{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Example usecase with Sentiment Analysis\n",
    "\n",
    "Using bayes theorem to classify text into positive, neutral, or negative\n",
    "\n",
    "$$\n",
    "P(positive | texts) = \\frac{P(texts | positive) * P(positive)}{P(texts)}\n",
    "\\\\\n",
    "P(neutral | texts) = \\frac{P(texts | neutral) * P(neutral)}{P(texts)}\n",
    "\\\\\n",
    "P(negative | texts) = \\frac{P(texts | negative) * P(negative)}{P(texts)}\n",
    "$$\n",
    "\n",
    "Where $P(positive)$, $P(neutral)$ and $P(negative)$ are the prior probabilities of the classes, equivalent to the fraction of the training set that belongs to each class.\n",
    "\n",
    "$P(texts | positive)$ is the likelihood of the texts given it's from the positive class, and so on for the other classes.\n",
    "\n",
    "Given $P(texts)$, the denominator, is constant for all classes, we can ignore it and just compare the numerators.\n",
    "\n",
    "Assuming independence in the occurence of words, we can calculate the likelihood of the texts given it's from the positive class as (Same for negative & neutral classes):\n",
    "\n",
    "$$\n",
    "P(texts | positive) = \\prod_{i=1}^{n} P(word_i | positive)\n",
    "$$\n",
    "\n",
    "We should take the log of the likelihoods to avoid underflow because of the multiplication of many small probabilities. The logs will still preserve the relative magnitude of the likelihoods.\n",
    "\n",
    "$$\n",
    "logP(texts | positive) = \\sum_{i=1}^{n} logP(word_i | positive)\n",
    "$$\n",
    "\n",
    "Now for the likelihood of each word given the class, we calculate it as follows:\n",
    "\n",
    "$$\n",
    "P(word_i | positive) = \\frac{count(text\\:with\\:word_i, positive)}{count(positive)}\n",
    "$$\n",
    "\n",
    "And, set $P(word_i | positive) = 1$ if $word_i$ does not appear in the training set. This assumes each word appears at least once in all of the positive or negative texts. Another way to handle this is to use Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/samridhashrestha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/samridhashrestha/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/samridhashrestha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from typing import Iterable, Callable\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/samridhashrestha/.cache/kagglehub/datasets/abhi8923shriv/sentiment-analysis-dataset/versions/9\n"
     ]
    }
   ],
   "source": [
    "# Download sentiment analysis dataset\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"abhi8923shriv/sentiment-analysis-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = path + \"/train.csv\"\n",
    "test_path = path + \"/test.csv\"\n",
    "\n",
    "assert os.path.exists(train_path)\n",
    "assert os.path.exists(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27481, 10) Index(['textID', 'text', 'selected_text', 'sentiment', 'Time of Tweet',\n",
      "       'Age of User', 'Country', 'Population -2020', 'Land Area (Km²)',\n",
      "       'Density (P/Km²)'],\n",
      "      dtype='object')\n",
      "(4815, 9) Index(['textID', 'text', 'sentiment', 'Time of Tweet', 'Age of User',\n",
      "       'Country', 'Population -2020', 'Land Area (Km²)', 'Density (P/Km²)'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_path, encoding=\"latin1\")\n",
    "test_df = pd.read_csv(test_path, encoding=\"latin1\")\n",
    "\n",
    "print(train_df.shape, train_df.columns)\n",
    "print(test_df.shape, test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment Time of Tweet Age of User  \\\n",
       "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
       "1                             Sooo SAD  negative          noon       21-30   \n",
       "2                          bullying me  negative         night       31-45   \n",
       "3                       leave me alone  negative       morning       46-60   \n",
       "4                        Sons of ****,  negative          noon       60-70   \n",
       "\n",
       "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
       "0  Afghanistan          38928346         652860.0               60  \n",
       "1      Albania           2877797          27400.0              105  \n",
       "2      Algeria          43851044        2381740.0               18  \n",
       "3      Andorra             77265            470.0              164  \n",
       "4       Angola          32866272        1246700.0               26  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27481, 2) Index(['text', 'sentiment'], dtype='object')\n",
      "(4815, 2) Index(['text', 'sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df[[\"text\", \"sentiment\"]]\n",
    "test_df = test_df[[\"text\", \"sentiment\"]]\n",
    "\n",
    "print(train_df.shape, train_df.columns)\n",
    "print(test_df.shape, test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(sentiment\n",
       " neutral     11118\n",
       " positive     8582\n",
       " negative     7781\n",
       " Name: count, dtype: int64,\n",
       " sentiment\n",
       " neutral     1430\n",
       " positive    1103\n",
       " negative    1001\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"sentiment\"].value_counts(), test_df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Dataset\n",
    "\n",
    "Remove all stopwords, punctuations, and convert all words to lowercase in the text column. Next, tokenize the text column.\n",
    "\n",
    "Convert sentiment column labels positive, neutral, and negative reviews to 1, 0 and -1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_TXT2NUM = {\"positive\": 1, \"negative\": -1, \"neutral\": 0}\n",
    "LABEL_NUM2TXT = {v: k for k, v in LABEL_TXT2NUM.items()}\n",
    "\n",
    "\n",
    "def preprocess_text(text_array):\n",
    "    \"\"\" \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Make a set with the stopwords and punctuation\n",
    "    stop = set(stopwords.words(\"english\") + list(string.punctuation))\n",
    "\n",
    "    if isinstance(text_array, str):\n",
    "        text_array = np.asarray([text_array])\n",
    "\n",
    "    X_preprocessed = []\n",
    "    for i, text in enumerate(text_array):\n",
    "        text = np.asarray(\n",
    "            [\n",
    "                lemmatizer.lemmatize(w.lower())\n",
    "                for w in word_tokenize(text)\n",
    "                if w.lower() not in stop\n",
    "            ]\n",
    "        ).astype(text_array.dtype)\n",
    "        X_preprocessed.append(text)\n",
    "\n",
    "    return X_preprocessed[0] if len(X_preprocessed) == 1 else X_preprocessed\n",
    "\n",
    "\n",
    "def preprocess_labels(y: Iterable):\n",
    "    \"\"\"\n",
    "    Maps sentiment labels ('positive', 'negative', 'neutral') to integers\n",
    "    (1, -1, 0) and converts them to a numpy array.\n",
    "\n",
    "    Args:\n",
    "        y (Iterable[str]): An iterable of sentiment labels.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of integers corresponding to the sentiment labels.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y = [LABEL_TXT2NUM[label] for label in y]\n",
    "    except KeyError as e:\n",
    "        raise ValueError(\n",
    "            f\"Invalid label found: {e.args[0]}. Allowed labels are {list(remap.keys())}.\"\n",
    "        )\n",
    "    return np.asarray(y).astype(int)\n",
    "\n",
    "\n",
    "def proprocess_df(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Processes a DataFrame with 'text' and 'sentiment' columns by preprocessing\n",
    "    the text data and mapping sentiment labels to integers.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A pandas DataFrame with columns 'text' and 'sentiment'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Preprocessed text (list of lists) and sentiment labels (numpy array).\n",
    "    \"\"\"\n",
    "    # Ensure required columns exist\n",
    "    if not {\"text\", \"sentiment\"}.issubset(df.columns):\n",
    "        raise ValueError(\"DataFrame must contain 'text' and 'sentiment' columns.\")\n",
    "    # Precompile regex patterns\n",
    "    url_pattern = re.compile(r\"http\\S+|www\\S+\")\n",
    "    number_pattern = re.compile(r\"\\d+\")\n",
    "    mention_pattern = re.compile(r\"@\\w+\")\n",
    "    hashtag_pattern = re.compile(r\"#\\w+\")\n",
    "\n",
    "    def clean_text(text):\n",
    "        \"\"\"\n",
    "        Cleans a single text string by:\n",
    "        - Removing URLs\n",
    "        - Removing numbers\n",
    "        - Converting emojis to text\n",
    "        - Removing mentions and hashtags\n",
    "        \"\"\"\n",
    "        text = url_pattern.sub(\"\", text)  # Remove URLs\n",
    "        text = number_pattern.sub(\"\", text)  # Remove numbers\n",
    "        text = emoji.demojize(text)  # Convert emojis to text\n",
    "        text = mention_pattern.sub(\"\", text)  # Remove @ mentions\n",
    "        text = hashtag_pattern.sub(\"\", text)  # Remove hashtags\n",
    "        return text.strip()\n",
    "\n",
    "    # Filter out non-string rows and clean text\n",
    "    df = df[df[\"text\"].apply(lambda x: isinstance(x, str))]\n",
    "    df.loc[:, \"text\"] = df[\"text\"].map(clean_text)\n",
    "\n",
    "    X, y = df[\"text\"], df[\"sentiment\"]\n",
    "    X, y = preprocess_text(X), preprocess_labels(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = proprocess_df(train_df)\n",
    "X_test, y_test = proprocess_df(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27480 (27480,)\n",
      "3534 (3534,)\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), y_train.shape)\n",
    "print(len(X_test), y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required calculations\n",
    "\n",
    "$$\n",
    "P(positive) = \\frac{number\\ of\\ positive\\ texts}{total\\ number\\ of\\ texts}\n",
    "\\\\\n",
    "logP(texts | positive) = \\sum_{i=1}^{n} logP(word_i | positive)\n",
    "\\\\\n",
    "P(word_i | positive) = \\frac{\\#\\ texts\\ containing\\ word_i\\ in\\ positive\\ class}{\\#\\ of\\ texts\\ in\\ positive\\ class}\n",
    "$$\n",
    "\n",
    "To eventually calculate:\n",
    "\n",
    "$$\n",
    "P(positive|text) = \\frac{P(text|positive)P(positive)}{P(text)}\n",
    "\\\\\n",
    "where\n",
    "\\\\\n",
    "P(text|positive) = P(word_0|positive)P(word_1|positive)...P(word_n|positive)P(positive)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_freq_dict(y: np.ndarray):\n",
    "    return {k: y[y == v].shape[0] for k, v in LABEL_TXT2NUM.items()}\n",
    "\n",
    "\n",
    "def get_word_freq_dict(X: np.ndarray, y: np.ndarray) -> dict:\n",
    "    word_freq_dict = {}\n",
    "    label2str_dict = {v: k for k, v in LABEL_TXT2NUM.items()}\n",
    "    for i, (text, label) in enumerate(zip(X, y)):\n",
    "        for word in text:\n",
    "            if word not in word_freq_dict:\n",
    "                word_freq_dict[word] = {\"positive\": 1, \"neutral\": 1, \"negative\": 1}\n",
    "            word_freq_dict[word][label2str_dict[label]] += 1\n",
    "\n",
    "    return word_freq_dict\n",
    "\n",
    "\n",
    "def get_prob_word_given_class(\n",
    "    word: str, cls: str, word_freq_dict: dict, class_freq_dict: dict\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the conditional probability of a given word occurring in a specific class.\n",
    "    P(word_i | class) = freq(word_i, class) / freq(class)\n",
    "\n",
    "    Assume each word appears at least once in each class.\n",
    "    \"\"\"\n",
    "    return word_freq_dict[word][cls] / class_freq_dict[cls]\n",
    "\n",
    "\n",
    "def get_prob_text_given_class(\n",
    "    text: list, cls: str, word_freq_dict: dict, class_freq_dict: dict\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the conditional probability of a given text occurring in a specific class.\n",
    "    P(text | class) = P(word_1 | class) * P(word_2 | class) * ... * P(word_n | class)\n",
    "    \"\"\"\n",
    "    prob = 1\n",
    "    for word in text:\n",
    "        # only compute for words in the word_freq_dict\n",
    "        if word in word_freq_dict:\n",
    "            prob *= get_prob_word_given_class(\n",
    "                word, cls, word_freq_dict, class_freq_dict\n",
    "            )\n",
    "    return prob\n",
    "\n",
    "\n",
    "def get_log_prob_text_given_class(\n",
    "    text: list, cls: str, word_freq_dict: dict, class_freq_dict: dict\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the log of the conditional probability of a given text occurring in a specific class.\n",
    "    log(P(text | class)) = log(P(word_1 | class)) + log(P(word_2 | class)) + ... + log(P(word_n | class))\n",
    "    \"\"\"\n",
    "    prob = 0\n",
    "    for word in text:\n",
    "        # only compute for words in the word_freq_dict\n",
    "        if word in word_freq_dict:\n",
    "            prob += np.log(\n",
    "                get_prob_word_given_class(word, cls, word_freq_dict, class_freq_dict)\n",
    "            )\n",
    "    return prob\n",
    "\n",
    "\n",
    "def clsf_sentiment_naive_bayes(\n",
    "    text: str, word_freq_dict: dict, class_freq_dict: dict, use_log: bool = False\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Classify the sentiment of a given text using the Naive Bayes algorithm.\n",
    "    \"\"\"\n",
    "    pred = {}\n",
    "    for cls in LABEL_TXT2NUM.keys():\n",
    "        if use_log:\n",
    "            log_p_text_given_cls = get_log_prob_text_given_class(\n",
    "                text, cls, word_freq_dict, class_freq_dict\n",
    "            )\n",
    "            log_p_cls = np.log(class_freq_dict[cls] / sum(class_freq_dict.values()))\n",
    "            log_p_text_given_cls_likelihood = log_p_text_given_cls + log_p_cls\n",
    "\n",
    "            pred[cls] = log_p_text_given_cls_likelihood\n",
    "        else:\n",
    "            p_text_given_cls = get_prob_text_given_class(\n",
    "                text, cls, word_freq_dict, class_freq_dict\n",
    "            )\n",
    "            p_cls = class_freq_dict[cls] / sum(class_freq_dict.values())\n",
    "            p_text_given_cls_likelihood = p_text_given_cls * p_cls\n",
    "\n",
    "            pred[cls] = p_text_given_cls_likelihood\n",
    "    pred_max_cls = max(pred, key=pred.get)\n",
    "\n",
    "    return {\n",
    "        \"pred_cls\": LABEL_TXT2NUM[pred_max_cls],\n",
    "        \"pred_cls_label\": pred_max_cls,\n",
    "        \"pred_cls_likelihood\": pred[pred_max_cls],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get class freq\n",
    "class_freq_dict = get_class_freq_dict(y_train)\n",
    "# get freq per class for each word\n",
    "word_freq_dict = get_word_freq_dict(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pred_cls': 0, 'pred_cls_label': 'neutral', 'pred_cls_likelihood': 1.2852100867013203e-39}\n",
      "{'pred_cls': 0, 'pred_cls_label': 'neutral', 'pred_cls_likelihood': np.float64(-89.54989643018753)}\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    clsf_sentiment_naive_bayes(\n",
    "        \"That movie was garbage\", word_freq_dict, class_freq_dict\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    clsf_sentiment_naive_bayes(\n",
    "        \"That movie was garbage\", word_freq_dict, class_freq_dict, use_log=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "\n",
    "def get_preds(X_test: np.ndarray, naive_bayes_classifier: Callable):\n",
    "    y_pred = []\n",
    "    for text in X_test:\n",
    "        y_pred.append(naive_bayes_classifier(text)[\"pred_cls\"])\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6406338426711942, 'precision': 0.6545125861963154, 'recall': 0.6406338426711942, 'f1': 0.6266208360574069}\n"
     ]
    }
   ],
   "source": [
    "clsf_sentiment_naive_bayes_test = partial(\n",
    "    clsf_sentiment_naive_bayes,\n",
    "    word_freq_dict=word_freq_dict,\n",
    "    class_freq_dict=class_freq_dict,\n",
    ")\n",
    "y_pred = get_preds(X_test, clsf_sentiment_naive_bayes_test)\n",
    "\n",
    "print(calculate_metrics(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6403508771929824, 'precision': 0.6542193305834327, 'recall': 0.6403508771929824, 'f1': 0.6262676456080541}\n"
     ]
    }
   ],
   "source": [
    "clsf_sentiment_naive_bayes_test = partial(\n",
    "    clsf_sentiment_naive_bayes,\n",
    "    word_freq_dict=word_freq_dict,\n",
    "    class_freq_dict=class_freq_dict,\n",
    "    use_log=True,\n",
    ")\n",
    "y_pred = get_preds(X_test, clsf_sentiment_naive_bayes_test)\n",
    "\n",
    "print(calculate_metrics(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
