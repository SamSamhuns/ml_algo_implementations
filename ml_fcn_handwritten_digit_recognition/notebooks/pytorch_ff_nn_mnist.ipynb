{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Pipeline of a NN PyTorch Project\n",
    "\n",
    "### Create Variables\n",
    "\n",
    "Inputs\n",
    "\n",
    "### Establish Networks (Funcs with variables)\n",
    "\n",
    "Outputs\n",
    "\n",
    "### Define Loss\n",
    "\n",
    "Loss = Target - Outputs\n",
    "\n",
    "### Calculate gradient to backpropagate Loss (Done by PyTorch)\n",
    "\n",
    "gradient\n",
    "\n",
    "### Update Weights with SGD or BGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N, D = 3, 4\\n\\nx = Variable(torch.randn(N, D).cuda(), requires_grad=True)\\ny = Variable(torch.randn(N, D).cuda(), requires_grad=True)\\nz = Variable(torch.randn(N, D).cuda(), requires_grad=True)\\n\\na = x * y\\nb = a + z\\nc = torch.sum(b)\\n\\nc.backward()\\nprint(x.grad.data)\\nprint(y.grad.data)\\nprint(z.grad.data)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"N, D = 3, 4\n",
    "\n",
    "x = Variable(torch.randn(N, D).cuda(), requires_grad=True)\n",
    "y = Variable(torch.randn(N, D).cuda(), requires_grad=True)\n",
    "z = Variable(torch.randn(N, D).cuda(), requires_grad=True)\n",
    "\n",
    "a = x * y\n",
    "b = a + z\n",
    "c = torch.sum(b)\n",
    "\n",
    "c.backward()\n",
    "print(x.grad.data)\n",
    "print(y.grad.data)\n",
    "print(z.grad.data)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Understand data with EDA (torch.utils.data.Datatset)\n",
    "\n",
    "    2. Implement model correctly (torch.nn)\n",
    "\n",
    "    3. Design Task specific Loss function (torch.nn), optimizer (torch.optim)\n",
    "\n",
    "    4. Training\n",
    "\n",
    "    4. Evaluation\n",
    "\n",
    "### For monitoring progress\n",
    "\n",
    "Tensorboardx\n",
    "\n",
    "lear.ai\n",
    "\n",
    "\n",
    "### Error Analysis and visualization\n",
    "\n",
    "Plot intermediate feature maps\n",
    "\n",
    "What if our network is not performing well for certain classes (Class imbalance problem)\n",
    "\n",
    "Solution: Give larger weights to class that perform poorly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a simple Neural Network in Pytorch without using torch.autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, transform=None):\n",
    "        \"\"\"\n",
    "        data should be a X,y tuple matrix\n",
    "        \"\"\"\n",
    "        self.X, self.y = data\n",
    "        self.X = torch.tensor(self.X, dtype=torch.float)\n",
    "\n",
    "        # Normalize data\n",
    "        mean = self.X.mean(0)\n",
    "        std = self.X.std(0)\n",
    "        std[std == 0] = 1.\n",
    "        self.X -= mean\n",
    "        self.X /= std\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns one data sample (X[index,:]) and its label (y[index])\n",
    "        \"\"\"\n",
    "        return self.X[index, :], self.y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "random_seed= 42\n",
    "test_split = .1\n",
    "validation_split = .2\n",
    "shuffle_dataset = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download training data from sklearn.datasets\n",
    "mnist_data = MNISTDataset(load_digits(return_X_y=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating splits for train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(mnist_data)\n",
    "indices = list(range(dataset_size))\n",
    "val_split_idx = int(np.floor(validation_split * dataset_size))\n",
    "test_split_idx = val_split_idx + int(np.floor(test_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "val_indices = indices[:val_split_idx]\n",
    "test_indices = indices[val_split_idx:test_split_idx]\n",
    "train_indices = indices[test_split_idx:]\n",
    "\n",
    "# Creating data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_data, batch_size=batch_size,\n",
    "                                           sampler=train_sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(mnist_data, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_data, batch_size=batch_size,\n",
    "                                          sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define networks structure, loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our neural network, structure is [64, 30, 10]\n",
    "neural_network1 = nn.Sequential(nn.Linear(64, 30),\n",
    "                              nn.Sigmoid(),\n",
    "                              nn.Linear(30, 10),\n",
    "                              nn.LogSoftmax(dim=1))\n",
    "\n",
    "# Loss function, Negative Log Loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = torch.optim.SGD(neural_network1.parameters(), lr=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our Neural Network with structure [64, 30, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss on epoch 0 is 2.25470625437223\n",
      "Training loss on epoch 100 is 0.05213634469188177\n",
      "Training loss on epoch 200 is 0.02237305023635809\n",
      "Training loss on epoch 300 is 0.013193730551462907\n",
      "Training loss on epoch 400 is 0.009187389487543931\n",
      "Training loss on epoch 500 is 0.0069558113598479675\n",
      "Training loss on epoch 600 is 0.005636111224213472\n",
      "Training loss on epoch 700 is 0.004793570448572819\n",
      "Training loss on epoch 800 is 0.003999811221057406\n",
      "Training loss on epoch 900 is 0.003470554553831999\n",
      "Training loss on epoch 1000 is 0.0031158817000687122\n",
      "Training loss on epoch 1100 is 0.002860109796389364\n",
      "Training loss on epoch 1200 is 0.002502961684233294\n",
      "Training loss on epoch 1300 is 0.0022827921650157524\n",
      "Training loss on epoch 1400 is 0.00211349711753428\n",
      "Training loss on epoch 1500 is 0.0019778714049607515\n",
      "Training loss on epoch 1600 is 0.0017828032052001129\n",
      "Training loss on epoch 1700 is 0.0016803136184954871\n",
      "Training loss on epoch 1800 is 0.0016216099477158142\n",
      "Training loss on epoch 1900 is 0.0014775800429141293\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "loss_overtime = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        # flatten MNIST images into a long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        # Training Pass, rest gradient to zero for each training pass\n",
    "        optimizer.zero_grad()\n",
    "        output = neural_network1(images) # forwardprop to generate output tensor\n",
    "        loss = criterion(output, labels) # calculate NLL loss\n",
    "        loss.backward()  # backprop loss, gradients\n",
    "        optimizer.step() # gradient descent SGD update of weights\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    avg_running_loss = running_loss/len(train_loader)\n",
    "    loss_overtime.append(avg_running_loss)\n",
    "    if epoch % 100 == 0:\n",
    "        print(\n",
    "            f\"Training loss on epoch {epoch} is {avg_running_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the loss overtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_overtime)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting hyperparameters based on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the validation set is 97.49303621169916%\n"
     ]
    }
   ],
   "source": [
    "correct, total = 0, 0\n",
    "\n",
    "for images, labels in valid_loader:\n",
    "    output = neural_network1(images)\n",
    "    for y_pred_vect, y_actual in zip(output, labels):\n",
    "        _, num_idx = torch.max(y_pred_vect, 0)\n",
    "        correct += (1 if num_idx == y_actual else 0)\n",
    "        total += 1\n",
    "print(f\"The accuracy for the validation set is {(correct/total)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the test set is 97.76951672862454%\n"
     ]
    }
   ],
   "source": [
    "for images, labels in test_loader:\n",
    "    output = neural_network1(images)\n",
    "    for y_pred_vect, y_actual in zip(output, labels):\n",
    "        _, num_idx = torch.max(y_pred_vect, 0)\n",
    "        correct += (1 if num_idx == y_actual else 0)\n",
    "        total += 1\n",
    "print(f\"The accuracy for the test set is {(correct/total)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
