{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression on the Boston Housing dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging, sys\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "logging.basicConfig(stream=sys.stderr, level=logging.INFO)\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Boston Housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information on our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boston house prices dataset has 506 instances and for each instance, it has 13 attributes\n",
    "and one target value.\n",
    "\n",
    "-   CRIM per capita crime rate by town\n",
    "-   ZN proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "-   INDUS proportion of non-retail business acres per town\n",
    "-   CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "-   NOX nitric oxides concentration (parts per 10 million)\n",
    "-   RM average number of rooms per dwelling\n",
    "-   AGE proportion of owner-occupied units built prior to 1940\n",
    "-   DIS weighted distances to five Boston employment centres\n",
    "-   RAD index of accessibility to radial highways\n",
    "-   TAX full-value property-tax rate per \\$10,000\n",
    "-   PTRATIO pupil-teacher ratio by town B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "-   LSTAT % lower status of the population\n",
    "-   MEDV Median value of owner-occupied homes in $1000â€™s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X input shape is (506, 13), y target shape is (506,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X input shape is {X.shape}, y target shape is {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Class and Utility classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TrainType(Enum):\n",
    "    NORMAL_EQUATION = 0\n",
    "    GRADIENT_DESCENT = 1\n",
    "\n",
    "Model_Parameter = namedtuple(\n",
    "    'Model_Parameter', ('lambda_ridge, training_method, alpha, epochs')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def standardize(X, mean=None, std=None, inplace=False):\n",
    "        if mean is None:\n",
    "            mean = np.mean(X, axis=0)\n",
    "        if std is None:\n",
    "            std = np.std(X, axis=0)\n",
    "\n",
    "        std = np.where(std == 0, 1, std)\n",
    "        if inplace:\n",
    "            X -= mean\n",
    "            X /= std\n",
    "        else:\n",
    "            X = (X-mean)/std\n",
    "        return X\n",
    "\n",
    "    @staticmethod\n",
    "    def insert_bias_term(X):\n",
    "        bias_arr = np.ones(X.shape[0])\n",
    "        return np.c_[bias_arr, X]\n",
    "    \n",
    "    def standardize_save_state(self, X, mean=None, std=None, inplace=False):\n",
    "        if mean is None:\n",
    "            mean = np.mean(X, axis=0)\n",
    "        if std is None:\n",
    "            std = np.std(X, axis=0)\n",
    "\n",
    "        std = np.where(std == 0, 1, std)\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        if inplace:\n",
    "            X -= mean\n",
    "            X /= std\n",
    "        else:\n",
    "            X = (X-mean)/std\n",
    "        return X\n",
    "    \n",
    "    def fit(self, X, inplace=False):\n",
    "        if self.mean is None or self.std is None:\n",
    "            raise ValueError(\"Mean or std is not for the preprocessing object\")\n",
    "        if inplace:\n",
    "            X -= self.mean\n",
    "            X /= self.std\n",
    "        else:\n",
    "            X = (X-self.mean)/self.std\n",
    "        return X     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class LinearRegr:\n",
    "\n",
    "    __slots__ = ['theta']\n",
    "\n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return ' '.join([str(parm) for parm in np.ndarray.flatten(self.theta)])\n",
    "\n",
    "    def fit(self, X, y,\n",
    "            model_params=Model_Parameter(lambda_ridge=0,\n",
    "                                         training_method=TrainType.NORMAL_EQUATION,\n",
    "                                         alpha=0.5,\n",
    "                                         epochs=1000)):\n",
    "        \"\"\"\n",
    "        Fit/train the linear model\n",
    "        It has been assumed that the bias has been added to X\n",
    "        \"\"\"\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\n",
    "                f\"X shape {X.shape[0]} != y shape {y.shape[0]}. Dimensions not matching\")\n",
    "\n",
    "        if model_params.training_method == TrainType.NORMAL_EQUATION:\n",
    "            self._normal_equation_method(X, y, model_params)\n",
    "        elif model_params.training_method == TrainType.GRADIENT_DESCENT:\n",
    "            self._gradient_descent_method(X, y, model_params)\n",
    "        else:\n",
    "            raise ValueError(\"Model type not supplied\")\n",
    "\n",
    "    def _normal_equation_method(self, X, y, model_params):\n",
    "        # Feature Scaling is not required\n",
    "        # theta = (XtX + LE*)-1 . Xt.y\n",
    "        # Almost identity matrix E where the first row, first col elem is 0\n",
    "        # since we do not regularize the bias input, x0 = 1\n",
    "        lambda_ridge = model_params.lambda_ridge\n",
    "\n",
    "        E_start = np.identity(X.shape[1])\n",
    "        E_start[0][0] = 0\n",
    "        E_start *= lambda_ridge\n",
    "        X_t = np.matrix.transpose(X)\n",
    "\n",
    "        dot_Xt_X = np.dot(X_t, X)  # XtX\n",
    "        self.theta = np.dot(\n",
    "            np.dot(np.linalg.pinv(dot_Xt_X+E_start), X_t), y)\n",
    "\n",
    "    def _gradient_descent_method(self, X, y, model_params):\n",
    "        \"\"\"\n",
    "        WARNING Feature scaling should already be done for X\n",
    "        Batch Gradient Descent\n",
    "        \"\"\"\n",
    "        lambda_ridge, training_method, alpha, epochs = model_params\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        loss_overtime = []\n",
    "        m = y.shape[0]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            gradient_wout_regu = (1/m)*np.dot(\n",
    "                np.matrix.transpose(X), np.dot(X, self.theta)-y)\n",
    "            # 0th parameter/bias is not regularized\n",
    "            self.theta[0] = self.theta[0] - alpha*gradient_wout_regu[0]\n",
    "            gradient_with_regu = gradient_wout_regu + \\\n",
    "                ((lambda_ridge/m)*self.theta)\n",
    "            # All other parameters regularized\n",
    "            self.theta[1:] = self.theta[1:] - alpha*gradient_with_regu[1:]\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                current_loss = self.loss(X, y, lambda_ridge)\n",
    "                logging.info(\n",
    "                    f\"Current loss at epoch {epoch} is {current_loss}\")\n",
    "                loss_overtime.append(current_loss)\n",
    "\n",
    "        self.plot_loss_curve(loss_overtime, epochs)\n",
    "\n",
    "    def plot_loss_curve(self, loss_arr, iterations, log_mode=False):\n",
    "        if log_mode:\n",
    "            plt.semilogx(range(iterations), loss_arr)\n",
    "        else:\n",
    "            plt.plot(loss_arr)\n",
    "        plt.title(\"Loss function\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def mse_loss(X, y, theta, lambda_ridge=0):\n",
    "        \"\"\" Calculates the MSE loss for linear regression \"\"\"\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\n",
    "                f\"X shape {X.shape[0]} != y shape {y.shape[0]}. Dimensions not matching\")\n",
    "        elif X.shape[1] != theta.shape[0]:\n",
    "            raise ValueError(\n",
    "                f\"X shape {X.shape[1]} != theta shape {theta.shape[0]}. Dimensions not matching\")\n",
    "\n",
    "        X_theta_minus_y = np.dot(X, theta)-y\n",
    "        X_theta_minus_y_t = np.matrix.transpose(X_theta_minus_y)\n",
    "\n",
    "        return (1/(2*X.shape[0])) * (\n",
    "            (np.dot(X_theta_minus_y_t, X_theta_minus_y)) +\n",
    "            (lambda_ridge*np.dot(np.matrix.transpose(theta[1:]), theta[1:])))\n",
    "\n",
    "    @staticmethod\n",
    "    def predict_X(X, theta):\n",
    "        \"\"\"\n",
    "        Predict using the linear model\n",
    "        \"\"\"\n",
    "        if theta is None:\n",
    "            raise ValueError(\"Model has not been trained yet\")\n",
    "\n",
    "        # prediction = X*theta\n",
    "        return np.dot(X, theta)\n",
    "\n",
    "    @staticmethod\n",
    "    def score_X(X, y, theta):\n",
    "        \"\"\"\n",
    "        Returns the coefficient of determination\n",
    "        \"\"\"\n",
    "        if theta is None:\n",
    "            raise ValueError(\"Model has not been trained yet\")\n",
    "\n",
    "        y_mean = np.mean(y)\n",
    "        y_pred = LinearRegr.predict_X(X, theta)\n",
    "        ss_total = sum((y-y_mean)**2)  # total sum of squares\n",
    "        ss_res = sum((y-y_pred)**2)    # sum of squared residuals\n",
    "\n",
    "        return 1 - (ss_res / ss_total)\n",
    "\n",
    "    def loss(self, X, y, lambda_ridge=0):\n",
    "        \"\"\"\n",
    "        Calculates the current loss\n",
    "        \"\"\"\n",
    "        if self.theta is None:\n",
    "            raise ValueError(\"Model has not been trained yet\")\n",
    "\n",
    "        return LinearRegr.mse_loss(X, y, self.theta, lambda_ridge)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns the coefficient of determination\n",
    "        \"\"\"\n",
    "        return LinearRegr.score_X(X, y, self.theta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict using the linear model\n",
    "        \"\"\"\n",
    "        return LinearRegr.predict_X(X, self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     24,
     61
    ]
   },
   "outputs": [],
   "source": [
    "class KFoldCrossValidator:\n",
    "\n",
    "    __slots__ = ['train_loss', 'test_loss', 'theta']\n",
    "\n",
    "    def __init__(self):\n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        self.theta = None\n",
    "\n",
    "    def cross_validate(self,\n",
    "                       model,\n",
    "                       model_params,\n",
    "                       X, y, k=10,\n",
    "                       custom_kfold=False,\n",
    "                       seed=np.random.randint(10000)):\n",
    "        \"\"\"\n",
    "        Cross validation function, the theta parameter chosen is from the split with the least test error\n",
    "        \"\"\"\n",
    "\n",
    "        m = X.shape[0]\n",
    "        lambda_ridge, training_method, alpha, epochs = model_params\n",
    "        min_test_error = float('inf')  # tracks the minimum error with k-folds\n",
    "        best_fit_theta = None  # saves the best theta value with the min_test_error\n",
    "\n",
    "        if custom_kfold:\n",
    "            logging.info(\n",
    "                f\"Running Custom KFoldCrossValidator with {k} folds and lambda={lambda_ridge}\")\n",
    "            np.random.seed(seed)  # seed random shuffler\n",
    "            if m < k:\n",
    "                raise ValueError(\n",
    "                    f\"No of k splits {k} cannot be greater than no. of samples {m}\")\n",
    "\n",
    "            # Randomly shuffle X and y inplace while matching corresponding feat and target\n",
    "            for i in range(m):\n",
    "                swap_idx = np.random.randint(i, m)\n",
    "                # ensures the corresponding feat-target values match\n",
    "                X[[i, swap_idx]] = X[[swap_idx, i]]\n",
    "                y[[i, swap_idx]] = y[[swap_idx, i]]\n",
    "\n",
    "            # test start and end idx\n",
    "            fold_step = m//k\n",
    "            start = 0\n",
    "            end = fold_step\n",
    "            for i in range(k):\n",
    "                end = min(end, m)  # prevent array idx out of bounds\n",
    "                X_train, X_test = np.concatenate(\n",
    "                    [X[0:start], X[end:m]], axis=0), X[start:end]\n",
    "                y_train, y_test = np.concatenate(\n",
    "                    [y[0:start], y[end:m]], axis=0), y[start:end]\n",
    "                start += fold_step\n",
    "                end += fold_step\n",
    "\n",
    "                model.fit(X_train, y_train, model_params)\n",
    "                cur_train_loss = model.loss(X_train, y_train, lambda_ridge)\n",
    "                cur_test_loss = model.loss(X_test, y_test, lambda_ridge)\n",
    "                self.train_loss.append(cur_train_loss)\n",
    "                self.test_loss.append(cur_test_loss)\n",
    "\n",
    "                if cur_test_loss < min_test_error:\n",
    "                    min_test_error = cur_test_loss\n",
    "                    best_fit_theta = model.theta\n",
    "        else:\n",
    "            logging.info(\n",
    "                f\"Running Sklearn KFoldCrossValidator with {k} folds and lambda {lambda_ridge}\")\n",
    "            kf = KFold(n_splits=k, random_state=seed, shuffle=True)\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "                model.fit(X_train, y_train, model_params)\n",
    "                cur_train_loss = model.loss(X_train, y_train, lambda_ridge)\n",
    "                cur_test_loss = model.loss(X_test, y_test, lambda_ridge)\n",
    "                self.train_loss.append(cur_train_loss)\n",
    "                self.test_loss.append(cur_test_loss)\n",
    "\n",
    "                if cur_test_loss < min_test_error:\n",
    "                    min_test_error = cur_test_loss\n",
    "                    best_fit_theta = model.theta\n",
    "        self.theta = best_fit_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running our regression model\n",
    "\n",
    "**Steps taken**\n",
    "\n",
    "-   Create a linear regression model class\n",
    "-   Create a KFoldCrossValidator class\n",
    "-   Add the bias feature\n",
    "-   Create a model parameter object with the hyperparameter values and the model type\n",
    "-   Run the cross validation function to get average loss on the train and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression model with no regularization i.e. lambda=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit a linear regression model using the closed form solution. Then use k-fold cross validation to estimate the performance of this model. Print the average of your recorded scores for both the test set and training set.\n",
    "\n",
    "**Average training and test loss with 10 folds Cross Validation and a lambda of 0 with the normal equation (Closed form solution)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running Sklearn KFoldCrossValidator with 10 folds and lambda 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss is: 10.905187135839643\n",
      "Average test loss is: 11.787812931304178\n",
      "R squared for the entire dataset is 0.7402547552453309\n"
     ]
    }
   ],
   "source": [
    "bh_model = LinearRegr()\n",
    "kfold_linear_regr = KFoldCrossValidator()\n",
    "X_feat = Preprocessing.insert_bias_term(X)\n",
    "\n",
    "model_params = Model_Parameter(lambda_ridge=0, training_method=TrainType.NORMAL_EQUATION, alpha=0, epochs=0)\n",
    "kfold_linear_regr.cross_validate(bh_model, model_params, X_feat, y, k=10, custom_kfold=False)\n",
    "\n",
    "lregr_train_loss = kfold_linear_regr.train_loss\n",
    "lregr_test_loss = kfold_linear_regr.test_loss\n",
    "print(f'Average train loss is: {sum(lregr_train_loss)/len(lregr_train_loss)}')\n",
    "print(f'Average test loss is: {sum(lregr_test_loss)/len(lregr_test_loss)}')\n",
    "\n",
    "print(f\"R squared for the entire dataset is {bh_model.score(X_feat, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression model with regularization. lambda parameters tested using K-Fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When lambda is 10.0\n",
      "Average train loss is: 11.573173093895647\n",
      "Average test loss is: 14.547718368863931\n",
      "\n",
      "When lambda is 31.622776601683793\n",
      "Average train loss is: 12.008203365913863\n",
      "Average test loss is: 16.8980895294772\n",
      "\n",
      "When lambda is 100.0\n",
      "Average train loss is: 12.805385012208115\n",
      "Average test loss is: 20.517681871381875\n",
      "\n",
      "When lambda is 316.22776601683796\n",
      "Average train loss is: 14.032687959712714\n",
      "Average test loss is: 24.842894898856844\n",
      "\n",
      "When lambda is 1000.0\n",
      "Average train loss is: 15.820055352459372\n",
      "Average test loss is: 31.506953974551795\n",
      "\n",
      "When lambda is 3162.2776601683795\n",
      "Average train loss is: 18.497448858769012\n",
      "Average test loss is: 41.55202619784027\n",
      "\n",
      "When lambda is 10000.0\n",
      "Average train loss is: 22.234452430655153\n",
      "Average test loss is: 51.25777463326127\n",
      "\n",
      "When lambda is 31622.776601683792\n",
      "Average train loss is: 26.117030701561685\n",
      "Average test loss is: 50.47470198003427\n",
      "\n",
      "When lambda is 100000.0\n",
      "Average train loss is: 28.837691733164384\n",
      "Average test loss is: 44.01005380124742\n",
      "\n",
      "When lambda is 316227.7660168379\n",
      "Average train loss is: 30.55540535331827\n",
      "Average test loss is: 41.24629977611433\n",
      "\n",
      "When lambda is 1000000.0\n",
      "Average train loss is: 31.953329881494405\n",
      "Average test loss is: 42.30658418621658\n",
      "\n",
      "When lambda is 3162277.6601683795\n",
      "Average train loss is: 33.64476374172396\n",
      "Average test loss is: 48.45695536504785\n",
      "\n",
      "When lambda is 10000000.0\n",
      "Average train loss is: 36.20746386678532\n",
      "Average test loss is: 56.99015222890595\n",
      "\n",
      "The lambda for ridge regression that yields the mininum error is 10.0\n",
      "R squared for the entire dataset is 0.7308523643988569\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.disabled = True\n",
    "\n",
    "possible_lambda_ridge = np.logspace(1, 7, num=13)\n",
    "min_test_error = float('inf')  # tracks the minimum error with k-folds\n",
    "best_lambda = None  # saves the best lambda value with the min_test_error\n",
    "best_theta = None   # saves the best theta value with the min_test_error\n",
    "\n",
    "for lambda_ridge in possible_lambda_ridge:\n",
    "    bh_model = LinearRegr()\n",
    "    kfold_linear_regr = KFoldCrossValidator()\n",
    "    X_feat = Preprocessing.insert_bias_term(X)\n",
    "\n",
    "    model_params = Model_Parameter(lambda_ridge=lambda_ridge, training_method=TrainType.NORMAL_EQUATION, alpha=0, epochs=0)\n",
    "    kfold_linear_regr.cross_validate(bh_model, model_params, X_feat, y, k=10, custom_kfold=False)\n",
    "\n",
    "    lregr_train_loss = kfold_linear_regr.train_loss\n",
    "    lregr_test_loss = kfold_linear_regr.test_loss\n",
    "    print(f\"When lambda is {lambda_ridge}\")\n",
    "    avg_train_loss, avg_test_loss = sum(lregr_train_loss)/len(lregr_train_loss), sum(lregr_test_loss)/len(lregr_test_loss)\n",
    "    print(f'Average train loss is: {avg_train_loss}')\n",
    "    print(f'Average test loss is: {avg_test_loss}')\n",
    "    print()\n",
    "    \n",
    "    if avg_test_loss < min_test_error:\n",
    "        min_test_error = avg_test_loss\n",
    "        best_lambda = lambda_ridge\n",
    "        best_theta = bh_model.theta\n",
    "\n",
    "print(f\"The lambda for ridge regression that yields the mininum error is {best_lambda}\")\n",
    "print(f\"R squared for the entire dataset is {LinearRegr.score_X(X_feat, y, best_theta)}\")\n",
    "logger.disabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lambda of 10 yields the best results quantified by the minimum test error, that is: \n",
    "\n",
    "-   Average train loss is: 11.56861911239611\n",
    "-   Average test loss is: 14.642007589269346"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression extended to Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_degree_transform = PolynomialFeatures(2)\n",
    "X_feat_poly = two_degree_transform.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average training and test loss with 10 folds Cross Validation and a lambda of 0 with the normal equation (Closed form solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running Sklearn KFoldCrossValidator with 10 folds and lambda 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss is: 2.9982571275911836\n",
      "Average test loss is: 7.583571795125268\n",
      "R squared for the entire dataset is 0.9214961827244105\n"
     ]
    }
   ],
   "source": [
    "bh_poly_model = LinearRegr()\n",
    "kfold_poly_regr = KFoldCrossValidator()\n",
    "\n",
    "poly_model_params = Model_Parameter(lambda_ridge=0, training_method=TrainType.NORMAL_EQUATION, alpha=0, epochs=0)\n",
    "kfold_poly_regr.cross_validate(bh_poly_model, poly_model_params, X_feat_poly, y, k=10, custom_kfold=False)\n",
    "\n",
    "pregr_train_loss = kfold_poly_regr.train_loss\n",
    "pregr_test_loss = kfold_poly_regr.test_loss\n",
    "print(f'Average train loss is: {sum(pregr_train_loss)/len(pregr_train_loss)}')\n",
    "print(f'Average test loss is: {sum(pregr_test_loss)/len(pregr_test_loss)}')\n",
    "print(f\"R squared for the entire dataset is {bh_poly_model.score(X_feat_poly, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression model with regularization. lambda parameters tested using K-Fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When lambda is 10.0\n",
      "Average train loss is: 3.5688836719411916\n",
      "Average test loss is: 8.861704861858973\n",
      "\n",
      "When lambda is 31.622776601683793\n",
      "Average train loss is: 3.794702043557172\n",
      "Average test loss is: 9.060617269450457\n",
      "\n",
      "When lambda is 100.0\n",
      "Average train loss is: 4.026295922630787\n",
      "Average test loss is: 9.1569351860967\n",
      "\n",
      "When lambda is 316.22776601683796\n",
      "Average train loss is: 4.27034329377639\n",
      "Average test loss is: 9.40068844560397\n",
      "\n",
      "When lambda is 1000.0\n",
      "Average train loss is: 4.544846042803544\n",
      "Average test loss is: 9.765489137321014\n",
      "\n",
      "When lambda is 3162.2776601683795\n",
      "Average train loss is: 4.832409382189917\n",
      "Average test loss is: 10.112162306092737\n",
      "\n",
      "When lambda is 10000.0\n",
      "Average train loss is: 5.125969050514972\n",
      "Average test loss is: 10.570889887669308\n",
      "\n",
      "When lambda is 31622.776601683792\n",
      "Average train loss is: 5.473409845191977\n",
      "Average test loss is: 11.234166599495888\n",
      "\n",
      "When lambda is 100000.0\n",
      "Average train loss is: 5.915900027002763\n",
      "Average test loss is: 12.060589692747646\n",
      "\n",
      "When lambda is 316227.7660168379\n",
      "Average train loss is: 6.564048649779903\n",
      "Average test loss is: 14.241461754148641\n",
      "\n",
      "When lambda is 1000000.0\n",
      "Average train loss is: 7.652648397064491\n",
      "Average test loss is: 18.59911454153492\n",
      "\n",
      "When lambda is 3162277.6601683795\n",
      "Average train loss is: 9.258355059004451\n",
      "Average test loss is: 22.837372091557516\n",
      "\n",
      "When lambda is 10000000.0\n",
      "Average train loss is: 11.00500703794609\n",
      "Average test loss is: 23.71994325550568\n",
      "\n",
      "The lambda for poylnomial regression that yields the mininum error is 10.0\n",
      "R squared for the entire dataset is 0.9159867990609393\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.disabled = True\n",
    "\n",
    "possible_lambda_ridge = np.logspace(1, 7, num=13)\n",
    "min_test_error = float('inf')  # tracks the minimum error with k-folds\n",
    "best_poly_lambda = None  # saves the lambda theta value with the min_test_error\n",
    "best_theta = None   # saves the best theta value with the min_test_error\n",
    "\n",
    "for lambda_ridge in possible_lambda_ridge:\n",
    "    bh_poly_model = LinearRegr()\n",
    "    kfold_poly_regr = KFoldCrossValidator()\n",
    "\n",
    "    poly_model_params = Model_Parameter(lambda_ridge=lambda_ridge, training_method=TrainType.NORMAL_EQUATION, alpha=0, epochs=0)\n",
    "    kfold_poly_regr.cross_validate(bh_poly_model, poly_model_params, X_feat_poly, y, k=10, custom_kfold=False)\n",
    "\n",
    "    pregr_train_loss = kfold_poly_regr.train_loss\n",
    "    pregr_test_loss = kfold_poly_regr.test_loss\n",
    "    \n",
    "    print(f\"When lambda is {lambda_ridge}\")\n",
    "    avg_train_loss, avg_test_loss = sum(pregr_train_loss)/len(pregr_train_loss), sum(pregr_test_loss)/len(pregr_test_loss)\n",
    "    print(f'Average train loss is: {avg_train_loss}')\n",
    "    print(f'Average test loss is: {avg_test_loss}')\n",
    "    print()\n",
    "    \n",
    "    if avg_test_loss < min_test_error:\n",
    "        min_test_error = avg_test_loss\n",
    "        best_poly_lambda = lambda_ridge\n",
    "        best_theta = bh_poly_model.theta\n",
    "\n",
    "print(f\"The lambda for poylnomial regression that yields the mininum error is {best_poly_lambda}\")\n",
    "print(f\"R squared for the entire dataset is {LinearRegr.score_X(X_feat_poly, y, best_theta)}\")\n",
    "logger.disabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of the better model\n",
    "\n",
    "From our results if we want to decide on our test set error results alone, the polynomial regression model with two polynomial features (i.e. form \\[a,b\\] generates \\[1, a, b, a^2, ab, b^2\\]) and a lambda of 10.0 generates the lowest test error and the highest R squared value of 0.91. However, this might just be a case of our model overfitting on our dataset due the high number of features and the very limited size of the training set (Only 506 values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "ml_svm_kernel",
   "language": "python",
   "name": "ml_svm_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
