{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classifier with Decision Tree using ID3 algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max normalization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def normalize(matrix):\n",
    "    \"\"\"\n",
    "    Min Max scaling\n",
    "    matrix must be a np.ndarry\n",
    "    \"\"\"\n",
    "    return (matrix - matrix.min(axis=0))/(matrix.max(axis=0) - matrix.min(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def entropy(y) -> float:\n",
    "    m = len(y)\n",
    "    class_freq_dict = Counter(y)\n",
    "    entropy = -1 * sum([(class_count/m)*np.log2(class_count/m)\n",
    "                        for class_count in class_freq_dict.values()])\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to load csv file and return X features and y target matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_csv(file, unpack=True):\n",
    "    data = np.genfromtxt(file, delimiter=',', encoding=None, dtype=str)\n",
    "    X, y = data[:, :-1], data[:, -1].reshape(-1,1)\n",
    "    return (np.array(X).astype(np.float), np.array(y)) if unpack else data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to randomly shuffle the array using the numpy.random.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def shuffle_array(matrix, seed=None):\n",
    "    \"\"\"\n",
    "    np.random.shuffle() shuffles a multidimensional arr using only the first dimension\n",
    "    Alternative implementation of shuffle\n",
    "    \n",
    "    def shuffle_array(matrix):\n",
    "        # shuffles an array in place based on the first dimension\n",
    "        for i in range(len(matrix)):\n",
    "            rd_idx = np.random.randint(i,len(matrix))\n",
    "            matrix[i], matrix[rd_idx] = matrix[rd_idx], matrix[i] \n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate k fold cross validation training and testing lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def generate_kfold_train_test_set(X, y, test_frac=0.10):\n",
    "    \"\"\"\n",
    "    Generates kfold train test sets from X_feat, y_target\n",
    "    Returns X_train_list, y_train_list, X_test_list, y_test_list\n",
    "    \n",
    "    X must be np.array of ints or floats\n",
    "    y is the target or label array\n",
    "    \"\"\"\n",
    "    m = len(X)\n",
    "    norm_X = normalize(X)\n",
    "    test_data_count = int(test_frac*m)\n",
    "    start, end = 0, test_data_count\n",
    "\n",
    "    # sets to store features and labels of training and testing data\n",
    "    X_train_list, y_train_list, X_test_list, y_test_list = [], [], [], []\n",
    "    \n",
    "    # 10-fold cross-validation:\n",
    "    for _ in range(10):\n",
    "        X_test = norm_X[start:end]\n",
    "        X_train = np.concatenate([norm_X[:start], norm_X[end:]], axis=0)\n",
    "        \n",
    "        X_test_list.append(X_test)\n",
    "        X_train_list.append(X_train)\n",
    "        \n",
    "        y_test = y[start:end].flatten()\n",
    "        y_train = np.concatenate([y[:start], y[end:]], axis=0).flatten()\n",
    "        \n",
    "        y_test_list.append(y_test)\n",
    "        y_train_list.append(y_train)\n",
    "        \n",
    "        # update test set start and end pointers\n",
    "        start += test_data_count\n",
    "        end += test_data_count\n",
    "\n",
    "    # return the feature, label fold lists for both training and testing set.\n",
    "    return X_train_list, y_train_list, X_test_list, y_test_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate attr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_attrib_dict_with_class_labels(X, y):\n",
    "    \"\"\"\n",
    "    Builds a dict where,\n",
    "        the keys are the feature idx, \n",
    "        the values are feature value and corresponding feature labels two-element lists\n",
    "    \"\"\"\n",
    "    attr_dict = {}\n",
    "\n",
    "    for feat_idx in range(X.shape[1]):\n",
    "        feat_vals = X[:, feat_idx]\n",
    "        attr_list = []\n",
    "        \n",
    "        for data_idx, val in enumerate(feat_vals):\n",
    "            attr_list.append([val, y[data_idx]])\n",
    "\n",
    "        attr_dict[feat_idx] = attr_list\n",
    "\n",
    "    return attr_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Node Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "\n",
    "    def __init__(self, class_label, left, right, threshold):\n",
    "        self.class_label = class_label\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.class_label}, {self.left}, {self.right}, {self.threshold}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    def get_major_class_label(self, class_values):\n",
    "        try:\n",
    "            return Counter(class_values).most_common(1)[0][0]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "\n",
    "    def get_best_threshold_perm_sorting(self, attri_list):\n",
    "        \"\"\"\n",
    "        Calculates best threshold value for each feature\n",
    "        attri_list are the values of the attr_dict\n",
    "        WARNING: perm sorts the training data. Shuffling is discarded\n",
    "        \"\"\"\n",
    "        attri_list.sort()\n",
    "        orig_entropy = entropy([label for val, label in attri_list])\n",
    "        max_info_gain = 0\n",
    "        optimal_threshold = 0\n",
    "        label_after_split_list = []\n",
    "        optimal_left_idx_list, optimal_right_idx_list = [], []\n",
    "\n",
    "        for i in range(len(attri_list)-1):\n",
    "            if attri_list[i][1] == attri_list[i+1][1]:\n",
    "                continue\n",
    "                \n",
    "            # candidate threshold is the midpoint between cur data and next data\n",
    "            threshold = (attri_list[i][0] + attri_list[i+1][0])/2\n",
    "            # lists that store index, value less than threshold\n",
    "            lt_idx_list, lt_val_list = list(range(i+1)), [label for val, label in attri_list[:i+1]]\n",
    "            # lists that store index, value greater than threshold\n",
    "            gt_idx_list, gt_val_list = list(range(i+1,len(attri_list))), [label for val, label in attri_list[i+1:len(attri_list)]]\n",
    "\n",
    "            # calculate the entropy of the \"less\" list\n",
    "            entropy_lt_list = entropy(lt_val_list)\n",
    "            # calculate the entropy of the \"greater\" list\n",
    "            entropy_gt_list = entropy(gt_val_list)\n",
    "            # calculate the info gain using the formula\n",
    "            info_gain = orig_entropy - (\n",
    "                entropy_lt_list*(len(lt_val_list)/len(attri_list)) +\n",
    "                entropy_gt_list*(len(gt_val_list)/len(attri_list)))\n",
    "\n",
    "            # if current info gain > max info gan\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                optimal_threshold = threshold\n",
    "                optimal_left_idx_list = lt_idx_list\n",
    "                optimal_right_idx_list = gt_idx_list\n",
    "\n",
    "        return max_info_gain, optimal_threshold, optimal_left_idx_list, optimal_right_idx_list\n",
    "\n",
    "    def get_best_threshold(self, attri_list):\n",
    "        \"\"\"\n",
    "        Calculates best threshold value for each feature\n",
    "        attri_list are the values of the attr_dict\n",
    "        \"\"\"\n",
    "\n",
    "        # extract data vals and label vals from sorted attri_list\n",
    "        data_list, class_label_list = map(list, zip(*sorted(attri_list)))\n",
    "\n",
    "        orig_entropy = entropy(class_label_list)\n",
    "        max_info_gain = 0\n",
    "        optimal_threshold = 0\n",
    "        label_after_split_list = []\n",
    "        optimal_left_idx_list, optimal_right_idx_list = [], []\n",
    "\n",
    "        for i in range(len(data_list)-1):\n",
    "            # IMPORTANT speeding step. If the i and the i+1th class values are the same, don't split\n",
    "            if class_label_list[i] == class_label_list[i+1]:\n",
    "                continue\n",
    "\n",
    "            # candidate threshold is the midpoint between cur data and next data\n",
    "            threshold = (data_list[i] + data_list[i+1])/2\n",
    "            # lists that store index, value less than threshold\n",
    "            lt_idx_list, lt_val_list = [], []\n",
    "            # lists that store index, value greater than threshold\n",
    "            gt_idx_list, gt_val_list = [], []\n",
    "\n",
    "            # for each index and value in attri_list\n",
    "            for j, data_tuple in enumerate(attri_list):\n",
    "                data, label = data_tuple\n",
    "                # if value less or equal than the current theta:\n",
    "                if data < threshold:\n",
    "                    # update the \"less\" list of index and value\n",
    "                    lt_idx_list.append(j)\n",
    "                    lt_val_list.append(label)\n",
    "                else:\n",
    "                    # update the \"greater\" list of index and value\n",
    "                    gt_idx_list.append(j)\n",
    "                    gt_val_list.append(label)\n",
    "\n",
    "            # calculate the entropy of the \"less\" list\n",
    "            entropy_lt_list = entropy(lt_val_list)\n",
    "            # calculate the entropy of the \"greater\" list\n",
    "            entropy_gt_list = entropy(gt_val_list)\n",
    "            # calculate the info gain using the formula\n",
    "            info_gain = orig_entropy - (\n",
    "                entropy_lt_list*(len(lt_val_list)/len(attri_list)) +\n",
    "                entropy_gt_list*(len(gt_val_list)/len(attri_list)))\n",
    "\n",
    "            # if current info gain > max info gan\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                optimal_threshold = threshold\n",
    "                optimal_left_idx_list = lt_idx_list\n",
    "                optimal_right_idx_list = gt_idx_list\n",
    "\n",
    "        return max_info_gain, optimal_threshold, optimal_left_idx_list, optimal_right_idx_list\n",
    "\n",
    "    def get_best_feature(self, attr_dict):\n",
    "        \"\"\"\n",
    "        Method to select the optimum feature out of all the features.\n",
    "        For each feature, get the optimum threshold and information gain\n",
    "        \"\"\"\n",
    "        best_feat = None\n",
    "        best_info_gain = -1\n",
    "        best_threshold = 0\n",
    "        best_left_idx_list = []\n",
    "        best_right_idx_list = []\n",
    "\n",
    "        # get max_info_gain using each feat as split to get the best_feat to split on\n",
    "        for feat in attr_dict.keys():\n",
    "            info_gain, threshold, left_idx_list, right_idx_list = self.get_best_threshold(\n",
    "                attr_dict[feat])\n",
    "\n",
    "            if info_gain > best_info_gain:\n",
    "                best_feat = feat\n",
    "                best_threshold = threshold\n",
    "                best_info_gain = info_gain\n",
    "                best_left_idx_list = left_idx_list\n",
    "                best_right_idx_list = right_idx_list\n",
    "\n",
    "        return [best_feat, best_threshold, best_left_idx_list, best_right_idx_list]\n",
    "\n",
    "    def get_remainder_dict(self, attr_dict, index_split):\n",
    "        split_dict = {}\n",
    "        exclude_index_split = set(index_split)\n",
    "\n",
    "        for feat in attr_dict.keys():\n",
    "            class_label_list = []\n",
    "            modified_list = []\n",
    "            feat_val_list = attr_dict[feat]\n",
    "\n",
    "            # feat_val_list = [[1.2,'iris-setosa'],[2.2,'iris-verisicolor'],....]\n",
    "            for i, dl_tuple in enumerate(feat_val_list):\n",
    "                if i in index_split:\n",
    "                    class_label_list.append(dl_tuple[1])\n",
    "                    modified_list.append(dl_tuple)\n",
    "            split_dict[feat] = modified_list\n",
    "\n",
    "        return split_dict, class_label_list\n",
    "\n",
    "    def create_decision_tree(self, attr_dict, y_train, n_min_val):\n",
    "        \"\"\"\n",
    "        Grow decision tree and return the root node\n",
    "        \"\"\"\n",
    "        # if all the class labels are same, node is pure\n",
    "        if len(set(y_train)) == 1:\n",
    "            return Node(y_train[0], None, None, 0)\n",
    "        # if num class vales are less than threshold, we assign the class with max values as the class label\n",
    "        elif len(y_train) < n_min_val:\n",
    "            max_value_class = self.get_major_class_label(y_train)\n",
    "            return Node(max_value_class, None, None, 0)\n",
    "        else:\n",
    "            feat, threshold, left_idx_split, right_idx_split = self.get_best_feature(\n",
    "                attr_dict)\n",
    "\n",
    "            left_tree_dict, left_tree_class_labels = self.get_remainder_dict(\n",
    "                attr_dict, left_idx_split)\n",
    "            right_tree_dict, right_tree_class_labels = self.get_remainder_dict(\n",
    "                attr_dict, right_idx_split)\n",
    "\n",
    "            left_node = self.create_decision_tree(\n",
    "                left_tree_dict, left_tree_class_labels, n_min_val)\n",
    "            right_node = self.create_decision_tree(\n",
    "                right_tree_dict, right_tree_class_labels, n_min_val)\n",
    "            return Node(feat, left_node, right_node, threshold)\n",
    "\n",
    "    def fit(self, X_train, y_train, eta_min_val):\n",
    "        attr_dict = get_attrib_dict_with_class_labels(X_train, y_train)\n",
    "\n",
    "        self.root = self.create_decision_tree(\n",
    "            attr_dict, y_train, eta_min_val)\n",
    "\n",
    "    def classify(self, data_row):\n",
    "        cur_node = self.root\n",
    "        #  while cur_node is not a leaf node\n",
    "        while cur_node and (cur_node.left or cur_node.right):\n",
    "            if data_row[cur_node.class_label] <= cur_node.threshold:\n",
    "                cur_node = cur_node.left\n",
    "            else:\n",
    "                cur_node = cur_node.right\n",
    "        return cur_node.class_label\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts labels for the X_feat data\n",
    "        \"\"\"\n",
    "        return [self.classify(data_row) for data_row in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_actual, y_pred):\n",
    "    return len([1 for actual,pred in zip(y_actual, y_pred) if actual==pred])/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def run_decision_trees(X, y, n_min):\n",
    "    \"\"\"\n",
    "    function to run decision trees with k-Fold Cross validation\n",
    "    \"\"\"\n",
    "    m, n = X.shape[0], X.shape[1]\n",
    "    \n",
    "    if isinstance(n_min, int):\n",
    "        n_min_val = n_min          # Use values for node cutoff\n",
    "    elif 0.0 < n_min < 1.0:\n",
    "        n_min_val = round(n_min*m) # Use fractions for node cutoff\n",
    "    else:\n",
    "        raise ValueError(\"Fractions must be in range (0.0, 1.0)\")\n",
    "    \n",
    "    # concatenate, shuffle and split  X,y\n",
    "    concatenated_data = np.concatenate([X,y], axis=1)\n",
    "    shuffle_array(concatenated_data)\n",
    "    X,y = concatenated_data[:,:-1].astype('float'), concatenated_data[:,-1].reshape(-1,1)\n",
    "    \n",
    "    X_train_list, y_train_list, X_test_list, y_test_list = generate_kfold_train_test_set(\n",
    "        X, y)\n",
    "\n",
    "    decision_tree = DecisionTree()\n",
    "    accuracy_list = []\n",
    "\n",
    "    for i in range(10):\n",
    "        X_train, y_train = X_train_list[i], y_train_list[i]\n",
    "        X_test, y_test = X_test_list[i], y_test_list[i]\n",
    "\n",
    "        # Fit the decision tree model\n",
    "        decision_tree.fit(X_train, y_train, n_min_val)\n",
    "\n",
    "        y_pred = decision_tree.predict(X_test)\n",
    "        accuracy = get_accuracy(y_test, y_pred)\n",
    "\n",
    "        accuracy_list.append(accuracy)\n",
    "        print(\"Accuracy is \", \"{:.4f}\".format(accuracy))\n",
    "    print(\"Average accuracy across 10-cross validation when cutoff is\",\n",
    "          n_min, \"nodes is\", \"{:.4f}\".format(np.mean(accuracy_list)))\n",
    "    print(\"Standard deviation across 10-cross validation when cutoff is\",\n",
    "          n_min, \"nodes is\", \"{:.4f}\".format(np.std(accuracy_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.9333\n",
      "Accuracy is  1.0000\n",
      "Accuracy is  0.9333\n",
      "Accuracy is  1.0000\n",
      "Accuracy is  1.0000\n",
      "Accuracy is  0.9333\n",
      "Accuracy is  1.0000\n",
      "Accuracy is  0.9333\n",
      "Accuracy is  0.7333\n",
      "Accuracy is  0.9333\n",
      "Average accuracy across 10-cross validation when cutoff is 5 nodes is 0.9400\n",
      "Standard deviation across 10-cross validation when cutoff is 5 nodes is 0.0757\n",
      "Accuracy is  0.8667\n",
      "Accuracy is  1.0000\n",
      "Accuracy is  0.8667\n",
      "Accuracy is  1.0000\n",
      "Accuracy is  1.0000\n",
      "Accuracy is  1.0000\n",
      "Accuracy is  0.9333\n",
      "Accuracy is  0.9333\n",
      "Accuracy is  1.0000\n",
      "Accuracy is  1.0000\n",
      "Average accuracy across 10-cross validation when cutoff is 10 nodes is 0.9600\n",
      "Standard deviation across 10-cross validation when cutoff is 10 nodes is 0.0533\n",
      "Accuracy is  0.8000\n",
      "Accuracy is  1.0000\n",
      "Accuracy is  1.0000\n",
      "Accuracy is  0.8667\n",
      "Accuracy is  1.0000\n",
      "Accuracy is  0.8667\n",
      "Accuracy is  1.0000\n",
      "Accuracy is  1.0000\n",
      "Accuracy is  0.9333\n",
      "Accuracy is  0.9333\n",
      "Average accuracy across 10-cross validation when cutoff is 15 nodes is 0.9400\n",
      "Standard deviation across 10-cross validation when cutoff is 15 nodes is 0.0696\n",
      "Accuracy is  0.9333\n",
      "Accuracy is  1.0000\n",
      "Accuracy is  0.9333\n",
      "Accuracy is  0.7333\n",
      "Accuracy is  1.0000\n",
      "Accuracy is  1.0000\n",
      "Accuracy is  1.0000\n",
      "Accuracy is  0.9333\n",
      "Accuracy is  0.9333\n",
      "Accuracy is  0.8667\n",
      "Average accuracy across 10-cross validation when cutoff is 20 nodes is 0.9333\n",
      "Standard deviation across 10-cross validation when cutoff is 20 nodes is 0.0789\n",
      "CPU times: user 640 ms, sys: 23.4 ms, total: 663 ms\n",
      "Wall time: 888 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# n_min = minimum frac/num of examples for a node to become a leaf\n",
    "# n_min_list_frac = [0.05,0.10,0.15,0.20,0.25]\n",
    "n_min_list_val = [5, 10, 15, 20]\n",
    "iris_data = \"../data/iris.csv\"\n",
    "X,y = load_csv(iris_data)\n",
    "\n",
    "for n_min in n_min_list_val:\n",
    "    run_decision_trees(X, y, n_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# eta_min_list_val = [5, 10, 15, 20, 25]\n",
    "eta_min_list_val = [5]\n",
    "iris_data = \"../data/spambase.csv\"\n",
    "X,y = load_csv(iris_data)\n",
    "\n",
    "for eta_min in eta_min_list_val:\n",
    "    run_decision_trees(X, y, eta_min)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
