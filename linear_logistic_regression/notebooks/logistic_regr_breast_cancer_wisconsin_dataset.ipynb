{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with the Breast Cancer Wisconsin Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "logging.basicConfig(stream=sys.stderr, level=logging.INFO)\n",
    "plt.style.use(\"seaborn-darkgrid\")\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Breast Cancer Wisconsin Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information on the Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute information:\n",
    "\n",
    "-   radius (mean of distances from center to points on the perimeter)\n",
    "-   texture (standard deviation of gray-scale values)\n",
    "-   perimeter area\n",
    "-   smoothness (local variation in radius lengths)\n",
    "-   compactness (perimeter^2 / area - 1.0)\n",
    "-   concavity (severity of concave portions of the contour)\n",
    "-   concave points (number of concave portions of the contour)\n",
    "-   symmetry fractal dimension (“coastline approximation” - 1)\n",
    "\n",
    "The mean, standard error, and “worst” or largest (mean of the three largest values) of these\n",
    "features were computed for each image, resulting in 30 features. For instance, field 3 is Mean\n",
    "Radius, field 13 is Radius SE, field 23 is Worst Radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of X_feat is (569, 30) and y_target is (569,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dimension of X_feat is {X.shape} and y_target is {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Class and Utility Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameter = namedtuple(\"model_parameter\", (\"lambda_ridge, alpha, epochs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    @staticmethod\n",
    "    def standardize(X, mean=None, std=None, inplace=False):\n",
    "        if mean is None:\n",
    "            mean = np.mean(X, axis=0)\n",
    "        if std is None:\n",
    "            std = np.std(X, axis=0)\n",
    "\n",
    "        std = np.where(std == 0, 1, std)\n",
    "        if inplace:\n",
    "            X -= mean\n",
    "            X /= std\n",
    "        else:\n",
    "            X = (X - mean) / std\n",
    "        return X\n",
    "\n",
    "    @staticmethod\n",
    "    def insert_bias_term(X):\n",
    "        bias_arr = np.ones(X.shape[0])\n",
    "        return np.c_[bias_arr, X]\n",
    "\n",
    "    def standardize_save_state(self, X, mean=None, std=None, inplace=False):\n",
    "        if mean is None:\n",
    "            mean = np.mean(X, axis=0)\n",
    "        if std is None:\n",
    "            std = np.std(X, axis=0)\n",
    "\n",
    "        std = np.where(std == 0, 1, std)\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        if inplace:\n",
    "            X -= mean\n",
    "            X /= std\n",
    "        else:\n",
    "            X = (X - mean) / std\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, inplace=False):\n",
    "        if self.mean is None or self.std is None:\n",
    "            raise ValueError(\"Mean or std is not for the preprocessing object\")\n",
    "        if inplace:\n",
    "            X -= self.mean\n",
    "            X /= self.std\n",
    "        else:\n",
    "            X = (X - self.mean) / self.std\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     44,
     56
    ]
   },
   "outputs": [],
   "source": [
    "class LogisticRegr:\n",
    "    slots = [\"theta\"]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \" \".join([str(val) for val in np.ndarray.flatten(self.theta)])\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        logging_enabled=True,\n",
    "        model_params=model_parameter(lambda_ridge=0, alpha=0.5, epochs=5000),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        WARNING: X must be normalized and have the bias term before training\n",
    "        Batch Gradient Descent for logistic regression\n",
    "        \"\"\"\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\n",
    "                f\"X shape {X.shape[0]} != y shape {y.shape[0]}. Dimensions not matching\"\n",
    "            )\n",
    "\n",
    "        loss_arr = []\n",
    "        m = X.shape[0]\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        lambda_ridge, alpha, epochs = model_params\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            gradient_wout_regu = (1 / m) * np.dot(\n",
    "                np.matrix.transpose(X), LogisticRegr.sigmoid(np.dot(X, self.theta)) - y\n",
    "            )\n",
    "            # 0th parameter/bias is not regularized\n",
    "            self.theta[0] = self.theta[0] - alpha * gradient_wout_regu[0]\n",
    "            gradient_with_regu = gradient_wout_regu + ((lambda_ridge / m) * self.theta)\n",
    "            # All other parameters regularized\n",
    "            self.theta[1:] = self.theta[1:] - alpha * gradient_with_regu[1:]\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                current_log_loss = self.loss(X, y, lambda_ridge)\n",
    "                if logging_enabled:\n",
    "                    print(f\"Loss at epoch {epoch} is {current_log_loss}\")\n",
    "                loss_arr.append(current_log_loss)\n",
    "\n",
    "        if logging_enabled:\n",
    "            self.plot_loss_curve(loss_arr, epochs)\n",
    "\n",
    "    def plot_loss_curve(self, loss_arr, epochs, log_scale: bool = False):\n",
    "        if log_scale:\n",
    "            plt.semilogx(range(epochs), loss_arr)\n",
    "        else:\n",
    "            plt.plot(loss_arr)\n",
    "        plt.ylabel(\"log loss\")\n",
    "        plt.xlabel(\"Epoch (x100)\")\n",
    "        plt.title(\"Loss Overtime\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def log_loss(X, y, theta, lambda_ridge: float = 0.0):\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\n",
    "                f\"X shape {X.shape[0]} != y shape {y.shape[0]}. Dimensions not matching\"\n",
    "            )\n",
    "        elif X.shape[1] != theta.shape[0]:\n",
    "            raise ValueError(\n",
    "                f\"X shape {X.shape[1]} != theta shape {theta.shape[0]}. Dimensions not matching\"\n",
    "            )\n",
    "\n",
    "        m = X.shape[0]\n",
    "        h = LogisticRegr.sigmoid(np.dot(X, theta))\n",
    "        # loss J(theta) = -(1/m)*(yt*logh + (1-y)t*log(1-h)) + lambda/2m theta_t * theta\n",
    "        return (-1 / m) * (\n",
    "            np.dot(np.matrix.transpose(y), np.log(h))\n",
    "            + np.dot(np.matrix.transpose(1 - y), np.log(1 - h))\n",
    "        ) + (\n",
    "            (lambda_ridge / (2 * m)) * np.dot(np.matrix.transpose(theta[1:]), theta[1:])\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    @staticmethod\n",
    "    def predict(X, theta, threshold: float = 0.5):\n",
    "        prediction = np.dot(X, theta)\n",
    "        prediction[prediction >= threshold] = 1\n",
    "        prediction[prediction < threshold] = 0\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, X, y, lambda_ridge: float = 0.0):\n",
    "        return LogisticRegr.log_loss(X, y, self.theta, lambda_ridge)\n",
    "\n",
    "    def accuracy(self, X, y, threshold: float = 0.5):\n",
    "        \"\"\"\n",
    "        accuracy = (TP+TN) / (TP+FP+TN+FN)\n",
    "        \"\"\"\n",
    "        y_pred = LogisticRegr.predict(X, self.theta, threshold)\n",
    "        return (\n",
    "            len([1 for y_true, y_hat in zip(y, y_pred) if y_true == y_hat]) / X.shape[0]\n",
    "        )\n",
    "\n",
    "    def precision(self, X, y, threshold: float = 0.5):\n",
    "        \"\"\"\n",
    "        Ratio of correctly predicted positive observations to total positive observations\n",
    "        precision = TP / (TP+FP)\n",
    "        \"\"\"\n",
    "        y_pred = LogisticRegr.predict(X, self.theta, threshold)\n",
    "        true_positives = len(\n",
    "            [1 for y_true, y_hat in zip(y, y_pred) if y_true == y_hat == 1]\n",
    "        )\n",
    "        total_positives_pred = sum(y_pred)\n",
    "        return true_positives / total_positives_pred\n",
    "\n",
    "    def recall(self, X, y, threshold: float = 0.5):\n",
    "        \"\"\"\n",
    "        Also known as Sensitivity\n",
    "        Ratio of correctly predicted positive observations to all observations that are actually positive\n",
    "        recall = TP / (TP+FN)\n",
    "        \"\"\"\n",
    "        y_pred = LogisticRegr.predict(X, self.theta, threshold)\n",
    "        true_positives = len(\n",
    "            [1 for y_true, y_hat in zip(y, y_pred) if y_true == y_hat == 1]\n",
    "        )\n",
    "        true_pos_and_false_neg = len(\n",
    "            [\n",
    "                1\n",
    "                for y_true, y_hat in zip(y, y_pred)\n",
    "                if y_true == y_hat == 1 or (y_true == 1 and y_hat == 0)\n",
    "            ]\n",
    "        )\n",
    "        return true_positives / true_pos_and_false_neg\n",
    "\n",
    "    def f1_score(self, X, y, threshold: float = 0.5):\n",
    "        \"\"\"\n",
    "        Weighted average of precision and recall\n",
    "        Preferred to accuracy as accuracy is misleading for unbalanced datasets\n",
    "        f1_score = 2*(Recall*Precision) / (Recall+Precision)\n",
    "        \"\"\"\n",
    "        recall = self.recall(X, y, threshold)\n",
    "        precision = self.precision(X, y, threshold)\n",
    "        return (2 * recall * precision) / (recall + precision)\n",
    "\n",
    "    def plot_confusion_matrix(self, X, y, threshold: float = 0.5, custom: bool = True):\n",
    "        y_pred = LogisticRegr.predict(X, self.theta, threshold)\n",
    "\n",
    "        tp = len([1 for y_true, y_hat in zip(y, y_pred) if y_true == y_hat == 1])\n",
    "        tn = len([1 for y_true, y_hat in zip(y, y_pred) if y_true == y_hat == 0])\n",
    "        fp = len([1 for y_true, y_hat in zip(y, y_pred) if y_true == 0 and y_hat == 1])\n",
    "        fn = len([1 for y_true, y_hat in zip(y, y_pred) if y_true == 1 and y_hat == 0])\n",
    "        if custom:  # use custom confusion matrix generator\n",
    "            print(\"\\t\\t\\t      Actual values\")\n",
    "            print(\"\\t\\t\\tPositive(1)   Negative(0)\")\n",
    "            print(f\"Predicted| Positive(1)     TP {tp}\\t  FP {fp}\")\n",
    "            print(f\"  Values | Negative(0)     FN {fn}\\t\\t  TN {tn}\")\n",
    "        else:  # use sklearn.metrics.confusion_matrix\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class KFoldCrossValidator:\n",
    "    __slots__ = [\"train_loss\", \"test_loss\", \"train_accuracy\", \"test_accuracy\", \"theta\"]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        self.train_accuracy = []\n",
    "        self.test_accuracy = []\n",
    "        self.theta = None\n",
    "\n",
    "    def cross_validate(\n",
    "        self,\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        k=10,\n",
    "        logging_enabled=True,\n",
    "        model_params=model_parameter(lambda_ridge=0, alpha=0.5, epochs=5000),\n",
    "        custom_kfold=False,\n",
    "        seed=np.random.randint(10000),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Cross validation function, the theta parameter chosen is from the split with the least test error\n",
    "        \"\"\"\n",
    "\n",
    "        m = X.shape[0]\n",
    "        lambda_ridge, alpha, epochs = model_params\n",
    "        min_test_error = float(\"inf\")  # tracks the minimum error with k-folds\n",
    "        best_fit_theta = None  # saves the best theta value with the min_test_error\n",
    "        preprocessor_object = Preprocessing()\n",
    "\n",
    "        if custom_kfold:\n",
    "            logging.info(\n",
    "                f\"Running Custom KFoldCrossValidator with {k} folds and lambda={lambda_ridge}\"\n",
    "            )\n",
    "            np.random.seed(seed)  # seed random shuffler\n",
    "            if m < k:\n",
    "                raise ValueError(\n",
    "                    f\"No of k splits {k} cannot be greater than no. of samples {m}\"\n",
    "                )\n",
    "\n",
    "            # Randomly shuffle X and y inplace while matching corresponding feat and target\n",
    "            for i in range(m):\n",
    "                swap_idx = np.random.randint(i, m)\n",
    "                # ensures the corresponding feat-target values match\n",
    "                X[[i, swap_idx]] = X[[swap_idx, i]]\n",
    "                y[[i, swap_idx]] = y[[swap_idx, i]]\n",
    "\n",
    "            # test start and end idx\n",
    "            fold_step = m // k\n",
    "            start = 0\n",
    "            end = fold_step\n",
    "\n",
    "            for i in range(k):\n",
    "                end = min(end, m)  # prevent array idx out of bounds\n",
    "                X_train, X_test = (\n",
    "                    np.concatenate([X[0:start], X[end:m]], axis=0),\n",
    "                    X[start:end],\n",
    "                )\n",
    "                y_train, y_test = (\n",
    "                    np.concatenate([y[0:start], y[end:m]], axis=0),\n",
    "                    y[start:end],\n",
    "                )\n",
    "                start += fold_step\n",
    "                end += fold_step\n",
    "\n",
    "                X_train = preprocessor_object.standardize_save_state(X_train)\n",
    "                X_test = preprocessor_object.fit(\n",
    "                    X_test\n",
    "                )  # standardizing X_test with X_train params\n",
    "\n",
    "                X_train = Preprocessing.insert_bias_term(X_train)\n",
    "                X_test = Preprocessing.insert_bias_term(X_test)\n",
    "\n",
    "                model.fit(X_train, y_train, logging_enabled, model_params)\n",
    "                cur_train_loss = model.loss(X_train, y_train, lambda_ridge)\n",
    "                cur_test_loss = model.loss(X_test, y_test, lambda_ridge)\n",
    "                self.train_loss.append(cur_train_loss)\n",
    "                self.test_loss.append(cur_test_loss)\n",
    "\n",
    "                if cur_test_loss < min_test_error:\n",
    "                    min_test_error = cur_test_loss\n",
    "                    best_fit_theta = model.theta\n",
    "        else:\n",
    "            logging.info(\n",
    "                f\"Running Sklearn KFoldCrossValidator with {k} folds and lambda {lambda_ridge}\"\n",
    "            )\n",
    "            kf = KFold(n_splits=k, random_state=seed, shuffle=True)\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "                X_train = preprocessor_object.standardize_save_state(X_train)\n",
    "                X_test = preprocessor_object.fit(\n",
    "                    X_test\n",
    "                )  # standardizing X_test with X_train params\n",
    "\n",
    "                X_train = Preprocessing.insert_bias_term(X_train)\n",
    "                X_test = Preprocessing.insert_bias_term(X_test)\n",
    "\n",
    "                model.fit(X_train, y_train, logging_enabled, model_params)\n",
    "                cur_train_loss = model.loss(X_train, y_train, lambda_ridge)\n",
    "                cur_test_loss = model.loss(X_test, y_test, lambda_ridge)\n",
    "                self.train_loss.append(cur_train_loss)\n",
    "                self.test_loss.append(cur_test_loss)\n",
    "\n",
    "                if cur_test_loss < min_test_error:\n",
    "                    min_test_error = cur_test_loss\n",
    "                    best_fit_theta = model.theta\n",
    "        self.theta = best_fit_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our Logistic regression model and plotting the objective loss function at every 100 iteration of gradient descent\n",
    "\n",
    "**NOTE** We use the log loss function so plot a gradient descent plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0 is 0.24583876431866866\n",
      "Loss at epoch 100 is 0.07843872123186416\n",
      "Loss at epoch 200 is 0.06872372926403282\n",
      "Loss at epoch 300 is 0.063834659516147\n",
      "Loss at epoch 400 is 0.06068011883471133\n",
      "Loss at epoch 500 is 0.058411311866183174\n",
      "Loss at epoch 600 is 0.05667270894909987\n",
      "Loss at epoch 700 is 0.05527319396712619\n",
      "Loss at epoch 800 is 0.054099023669670766\n",
      "Loss at epoch 900 is 0.05308189470043131\n",
      "Loss at epoch 1000 is 0.05217977595024646\n",
      "Loss at epoch 1100 is 0.05136553120349583\n",
      "Loss at epoch 1200 is 0.05062077451720506\n",
      "Loss at epoch 1300 is 0.04993253125857482\n",
      "Loss at epoch 1400 is 0.049291330052199406\n",
      "Loss at epoch 1500 is 0.048690054855306183\n",
      "Loss at epoch 1600 is 0.04812322409022393\n",
      "Loss at epoch 1700 is 0.0475865216243665\n",
      "Loss at epoch 1800 is 0.04707648229594629\n",
      "Loss at epoch 1900 is 0.04659027550413878\n",
      "Loss at epoch 2000 is 0.04612555285153405\n",
      "Loss at epoch 2100 is 0.04568033869953008\n",
      "Loss at epoch 2200 is 0.045252950126519045\n",
      "Loss at epoch 2300 is 0.044841937436917445\n",
      "Loss at epoch 2400 is 0.04444603929034601\n",
      "Loss at epoch 2500 is 0.044064148397221815\n",
      "Loss at epoch 2600 is 0.04369528495991017\n",
      "Loss at epoch 2700 is 0.043338575864834465\n",
      "Loss at epoch 2800 is 0.04299323819484783\n",
      "Loss at epoch 2900 is 0.042658566022419624\n",
      "Loss at epoch 3000 is 0.042333919719696286\n",
      "Loss at epoch 3100 is 0.04201871721809757\n",
      "Loss at epoch 3200 is 0.04171242679208688\n",
      "Loss at epoch 3300 is 0.04141456104537932\n",
      "Loss at epoch 3400 is 0.04112467185421775\n",
      "Loss at epoch 3500 is 0.040842346079108534\n",
      "Loss at epoch 3600 is 0.040567201898934986\n",
      "Loss at epoch 3700 is 0.04029888565345894\n",
      "Loss at epoch 3800 is 0.04003706910460326\n",
      "Loss at epoch 3900 is 0.039781447045554065\n",
      "Loss at epoch 4000 is 0.03953173520106921\n",
      "Loss at epoch 4100 is 0.039287668373487176\n",
      "Loss at epoch 4200 is 0.03904899879758448\n",
      "Loss at epoch 4300 is 0.03881549467421061\n",
      "Loss at epoch 4400 is 0.03858693885797915\n",
      "Loss at epoch 4500 is 0.03836312767853936\n",
      "Loss at epoch 4600 is 0.038143869878344595\n",
      "Loss at epoch 4700 is 0.03792898565256213\n",
      "Loss at epoch 4800 is 0.03771830577897841\n",
      "Loss at epoch 4900 is 0.03751167082755471\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwMAAAIiCAYAAACUpRQEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdeZhkdXn3/3f13jPTM8MwzbAvgtygIoKAiQq4xH0XA0kUg3H55XHJzyWJGowiKiAao0aNMcYF5EEjMQYxbiguGFSiAoLyBdkHBpitZ+3u6Z6u549zqqeqp6u7p6dmuqvO+3VdfZ2us1cfcepT93cplctlJEmSJBVP21zfgCRJkqS5YRiQJEmSCsowIEmSJBWUYUCSJEkqKMOAJEmSVFCGAUmSJKmgDAOSJElSQRkGJEmSpIIyDEiSJEkFZRiQJEmSCsowIEmSJBWUYUCSJEkqKMOAJEmSVFAdc30DktTqIuIpwDUAKaXS3N7N3hERC4C/AF4IPBroBwaBBPw38C8ppVVzd4czExHdwIEppbuq1p0DfB74ZUrppLm6N0lqBCsDkqSGiohnAbcD/wQ8I199I7ABOAl4D3BHRLx6bu5wZiLiGcDNwPPm+l4kaU8xDEiSGiYizgSuAg4kqwCckFI6KKV0ckrpUOBw4FNAL/DZiLhozm52eucCR02y/j+BY4Ez9u7tSFLj2UxIktQQERHA58j+bbk4pfT2ifuklO4F3hAR15M1tXl7RPwwpfTtvXu3s5dS2kBW5ZCkpmdlQJLUKO8FFgI/Ad4x1Y4ppS8Al+Yv/yUi/HJKkuaA/+crSfNYRCwF3gy8lKzJyhhZJ9wvA59IKQ1OcsxLgP9D1j5/EbAW+DnwqZTSd3d3/zr32Q+8LH/5Dyml8gwOew9wNnAo8Fzgyoh4L/Bu4KqU0gvqXOuLwCuZUH2IiCPIQsgzyZopbQJ+BvxjSun7E85xOHAXcAtwJvBvwAnAOuACsv4OFf8UEf8EvDeldF69DsQRUSarGOwD/GX+80hgI/Bt4G9TSg9HxAlkwelUoAe4ATg/pfStSd5rW/5e/wJ4LNCd3/d/AB/OqxSSNGtWBiRpnoqIR5J1vH0P8CjgNrIPgicAFwM/i4j9JxzzDuBrZB13B/Lj24AXAd+JiDfuzv5TOBVoJwsr359mXwDyEXp+k798Tr78Ur58ZkTsM/GYiOgBXpy/vLRq/bPyc70O2I/sQ/4gWeffqyPiPXVuYwnwHeAxwG+BpWQfuH9K9iEe4M789b0zeFul/D18iiwU3AEsB/4c+H5EPI8soDwduBsYAv4AuCoiTpvwXruBK8mCx6lkz+d3wJHAu4BfRcQjZnBPklSXYUCS5qGI6AS+Qfat+Y+Aw1NKj0spHQccA9xE9k3xv1cds4zsG+ch4PSU0iNSSieTfUt+br7bhfmwn7u8/zSOz5d3pZQ278JbvSFfPgYgpXQ72YflLrJqyETPBxYDN6aUbs7fx+Fkf4eFwPuAfVJKJ6aUDiELNRuB8yLixZOc72Cy0HBkSunE/PVHUkpPBn6d7/OPKaUnp5Q+N4P3s5is0vDKlNKh+fN6GlDO3+OVwFeB/VJKJwCHANeR/Xv85gnn+hBZmPkdcGJK6fD8Hg8kC3CPAL6aVw8kaVb8PxBJmp/+BAjgIeBFKaWVlQ0ppdvIPiRuAU6NiMq36keTfYhOKaUfV+2/PaV0AdkoOFcA+85y/6lU9lm3i+/z4XzZX7Wu8o3/n0yy/59O2Afgr8k+hF+SUnp3SmlbZUNK6Up29F+oVx34cErp4Xz/dTNs4jSVL6SUxu8v/9tel798ADgnpbQl37aZrIoA8LjKMRFxIFkzo23AS1NKv64631rg5WSVihOBSZtTSdJMGAYkaX6qjG1/2WTtwvNw8LX85fPz5d3AduD4iLhoYhOSlNJLU0qvSindN8v9p9KeL7dNudfORvNl9WRsX8nP89SI2K+yMiIWk/Ut2A7836r9X5gvL69zjS+TfTP/uInNqnLXTbJud+zU9h+4J19ek1IanbDtoXzZV7XuOUAn8OuU0q0TT5ZSGiILa5V9JWlW7EAsSfPT0fny11Ps8yuyDrhHA6SUHoyIjwNvAd5ONmzn7WRt4r8JfD+lNFI5eFf3n8b9+XKndv7TqFQU1lTd19qI+G+yvgF/DHwy3/RSsg6336vMXhwRfWRNbQAuiIh31bnOdrJ/8wJ4cMK2Rs+EfP8k6yohafUk2yp/4+pA9Kh8eVREXFvnOgfky9i125OkHawMSNL8VPmWeNMU+1Ta5o9/o5xSeivwCrJvu8tko9m8kezb6gci4rXVJ9jV/adwW748KiJ6Z3gM7OhrcPOE9ZM1FZqsidDiqt9PAJ5U56fy5deSSe5haBfudya2TLFtpk2QKu9rX+q/p0olZ7L3JEkzYmVAkuanygf9xVPsU/kQWNNhN6V0GXBZ3sTmaWQjBb2ArF3+ZyLi4ZTSf812/zq+Rfahugd4FvD16d5gRBxM9gEesmpEtW8C64En5fttIxuBZys7msdA7Qfv/pTSGlpD5X19MqU00xGdJGmXWRmQpPkp5csTptjn8fny9wARsSAiToiIYwBSSg+nlL6cUno1WVOaq/P9XzGb/ae82ZQ2kXU2Bvi7iGifav/c28m+lFpFNnJS9fmGyUYIKpGNCHQGWb+E/6werSilNMCOpjfHTHaRiGiPiD+KiKNmeF/zwe35ctL3BBARx0bESZMNwSpJM2UYkKT56Zv58s8iYqdmIPm35S/KX1YmBvsrsn4EH524f/7h+if5y/ZZ7j+dvyEbTehk4KMRUaq3Y0ScBbwhf/mOOn0TLsmXL2THMKOXTrLff+fLv6xzuZcD3yMbxnRR3bvf2Vi+rPs+9qBv59d/SkTs1Ccgn7H568D1wFv38r1JaiGGAUnaiyJi6TQ/PfmuXyGrDqwAvp5/+K+c42iysLCArK1/pQnPv5N1lH1WRPxN/oGxcsxjgP8vf/mtWe4/pZTSg2STaw2R9Tu4KiKOm/D+D4iIfyAbDagEfDqldMlOJ8vO9z9kk3Y9BTiNrOPv1ZPsenF+zZdHxAeq/oaVycg+kb/8112csbdSgThsF45piJTSHcBlZEHsqog4sbItrwR8iazj+Bbg03v7/iS1DsOAJO1d66f5eQdAPlb+i8nGkn8KcFdE3BARvwFuJZtw7CbgrJTS9vyYO8lGBoLsA/LDEfG/EXFbvu+BZCHi87PZfyZSSlcBpwMryYYBvSki7ouIX+TnvZ/sm+xR4O+A109zyi+RzYXQBVxeea8Trvlb4JXAcH7OhyPi+oi4m+wb9j6yEPH2mb6P3E358s0R8euIeOcuHr+73gD8GDgK+GVE3BYRvyL7255FNgrRGSmlyUYvkqQZMQxI0jyVjy//OOD9ZFWCo8na8l9PNlvtEybOAZBS+iey5kPfIvvAfTywnKzJz2uBF1aPc7+r+8/wvn+R3+vryT6Ed5P1fViR3/v5wCNSShfOYIKv6mZBX5riml8l+1v9G1lTpcfm76Pyt3pu9WRkM3QR8EWyGYyPIZ8leW/J+2H8EVmF5lpgP+A4stB4GXBSSmlix2tJ2iWlcnl3J1qUJEmS1IysDEiSJEkFZRiQJEmSCsowIEmSJBWUYUCSJEkqKMOAJEmSVFCGAUmSJKmgDAOSJElSQRkGJEmSpIIyDEiSJEkFZRiQJEmSCsowIEmSJBVUx1zfQCtbvXpTeS6uu3TpAgAGBrbOxeW1l/m8i8XnXTw+82LxeRdLI593f39faTbHWRmQJEmSCsowIEmSJBWUYUCSJEkqKMOAJEmSVFBN14E4IjqANwGvBY4AVgGfBy5KKY3M4PjHA38PnAr0AfcBXwXel1LaMmHfS4FX1DnVB1NK75jt+5AkSZLmWtOFAeCTwOuAa4ErgScB5wPHAy+b6sCIeCrw7fzlfwAPAKcBbweeFhGnpZSGqg45HngI+PQkp7t2N96DJEmSNOeaKgxExBPJgsAVwJkppXJElIAvAK+MiOenlK6a4hSfImsa9aSU0i/yc5aAfyGrNLwe+Ei+vhM4BrgqpXTennlHkiRJ0txptj4Db8iX700plQHy5TuBMvCaegdGxKPIPtz/VyUIVB1/fv7yOVWHHAt0Ajc17O4lSZKkeaTZwsBpwJqU0s3VK1NKDwC3AadPcexGsuZAn5tk23C+XFS17rH50jAgSZKkltQ0zYQiohs4GPh5nV3uznaL/pTS6okbU0orgYvrHPuSfHlL1bpKGIiI+Gn+ehD4JnBuHkAkSZKkptU0YQBYli8H6mzfkC+XADuFgXoiYgU7mgl9pmpTJQz8PfA14GfAE4BzgGdExB/kAaOuyhTTe1tHR9ucXl97l8+7WHzexeMzLxafd7HMh+fdTGGgM18O19leWd8z0xNGxBKyb/pXAB+v7ktAVgW4HXhJSumWqmPOBd4PfBx46UyvJUmSJM03zRQGBvNlV53t3flyS53tNSKin2yY0ROBq4C3VW9PKb1ksuOAC4FXAy+IiEUppc31rjEwsHUmt9JwlXQ5V9fX3uXzLhafd/H4zIvF510sjXze/f19szqumToQbwDGyJoBTWZJ1X5TiogjgevIgsCVwMtSSqMzuYmU0hhwI1mQOngmx0iSJEnzUdOEgZTSNuAeslmHJ3MEsDqltG6q80TE44D/AY4EvgickVIanrDPgoj4g4g4vs5pevPlUJ3tkiRJ0rzXNGEgdy2wf0QcXb0yIg4Ejibr5FtXRBwFfBfYj2xysVfVqQjsT1Y5uHSScywgqyisJgsnkiRJUlNqtjBwSb68ICLaYHwG4Qvz9Z+Z9KhsvzbgcqAf+FhK6W2VicsmSindCfwKOC4iXl51jhJwUX6Of653/Fwrl+flbUmSJGmeaaYOxKSUro6IrwBnAddFxDXAE4FTgSvIRgYCICLOy485L1/1YuAkslGHNle2T/BgSunT+e+vA34IXBoRZ5DNY3Bqfo4fAxc07p01zi2rNvKuz13PYcsW8KEXHEtne7PlPUmSJO0tTRUGcmeTTQ52DvBm4F7g3cDFE76pf0++PC9fnpYvu4Fz65z7RuDTACmlX0bEyWRzEDwNeB5ZIKhcq94Qp3PqP296kJXrB1m5fpDr7l7PaUfuO9e3JEmSpHmq6cJASmkEeF/+M9V+pQmv30wWHnblWrcCZ+7qPc6l0bGx8d83DI7M4Z1IkiRpvrMNSYvp6Wwf/31odGyKPSVJklR0hoEW092x45EOjWyfwzuRJEnSfGcYaDFWBiRJkjRThoEW01NTGTAMSJIkqT7DQIuprgwMj9pMSJIkSfUZBlqMlQFJkiTNlGGgxfR0VoUBKwOSJEmagmGgxfR0VHUgtjIgSZKkKRgGWoyVAUmSJM2UYaDFWBmQJEnSTBkGWkxtZcAwIEmSpPoMAy2mtjJgMyFJkiTVZxhoMVYGJEmSNFOGgRZTO8+AlQFJkiTVZxhoMdUzEFsZkCRJ0lQMAy2mu6oyMDw6xli5PId3I0mSpPnMMNBi2kqlmkCwzeqAJEmS6jAMtKDeTucakCRJ0vQMAy2ott+AnYglSZI0OcNAC+qtHl7UyoAkSZLqMAy0ICsDkiRJmgnDQAvq7bLPgCRJkqZnGGhBVgYkSZI0E4aBFuRoQpIkSZoJw0AL6qnuQOw8A5IkSarDMNCCqisDgyM2E5IkSdLkDAMtqKaZkJUBSZIk1WEYaEE1HYitDEiSJKkOw0ALsjIgSZKkmTAMtKCeLisDkiRJmp5hoAX1Vo0mNGxlQJIkSXUYBlqQfQYkSZI0E4aBFmSfAUmSJM2EYaAF9TgDsSRJkmbAMNCCFlR3IB61mZAkSZImZxhoQVYGJEmSNBOGgRZU22fAyoAkSZImZxhoQdVDi1oZkCRJUj2GgRbU42hCkiRJmgHDQAvqdZ4BSZIkzYBhoAVZGZAkSdJMGAZaUGd7ifa2EgDbx8qMbjcQSJIkaWeGgRZUKpXoqe5EbHVAkiRJkzAMtCj7DUiSJGk6hoEWZb8BSZIkTccw0KJ6nYVYkiRJ0zAMtChnIZYkSdJ0DAMtqsdZiCVJkjQNw0CLsjIgSZKk6RgGWlSPfQYkSZI0DcNAi7IyIEmSpOkYBlpUT5eVAUmSJE3NMNCiep2BWJIkSdMwDLSoHmcgliRJ0jQMAy2q1xmIJUmSNA3DQIuyMiBJkqTpGAZalJUBSZIkTccw0KJ6rQxIkiRpGoaBFtVTNZrQsJUBSZIkTcIw0KJ6nWdAkiRJ0zAMtKgeZyCWJEnSNAwDLaq2z4CVAUmSJO3MMNCirAxIkiRpOoaBFtVb1YHYyoAkSZImYxhoUc4zIEmSpOkYBlqUMxBLkiRpOoaBFtUzoTJQLpfn8G4kSZI0HxkGWlR7W4mu9tL4aycekyRJ0kSGgRY2sTogSZIkVTMMtLCejuoRhew3IEmSpFqGgRZmZUCSJElTMQy0sO6qysCwcw1IkiRpAsNAC+vpcBZiSZIk1WcYaGE9zkIsSZKkKRgGWlhNB2IrA5IkSZrAMNDCamchtjIgSZKkWoaBFmZlQJIkSVMxDLQwKwOSJEmaimGghdVWBgwDkiRJqtUx1zewqyKiA3gT8FrgCGAV8HngopTSyAyOfzzw98CpQB9wH/BV4H0ppS0T9l0AvBP4U+Ag4C7gk8CnUkrlRr2nPaV2NCGbCUmSJKlWM1YGPgl8BFgLfAy4HzgfuHy6AyPiqcD/AM8BvgN8PD/P24FrIqKnat92spDwLiDl1xoBPgF8qHFvZ8+pnWfAyoAkSZJqNVUYiIgnAq8DrgBOSym9AzgNuAQ4IyKeP80pPkX2nk9NKf1ZSumvgScA/wqcDLy+at+zgOcCH04pPS+/1knAD4C3RsRxDXxre4SVAUmSJE2lqcIA8IZ8+d5KM518+U6gDLym3oER8SjgGOC/Ukq/qKzPjz8/f/mcCdcaBS6o2neErFJQAl69u29mT7MyIEmSpKk0Wxg4DViTUrq5emVK6QHgNuD0KY7dSNYc6HOTbBvOl4sAIqIbOAW4IaW0fsK+vwC2TnOtecEZiCVJkjSVpulAnH9APxj4eZ1d7s52i/6U0uqJG1NKK4GL6xz7knx5S748jOxvc8ck59keEfcBR093z0uXLphulz2iIx9FaN8lO66/vTR396M9q/K8fb7F4PMuHp95sfi8i2U+PO9mqgwsy5cDdbZvyJdLduWkEbGCHc2EPpMv953BtRbkIxvNWz1d9hmQJElSffP6w+wEnflyuM72yvqeOtt3EhFLgG8CK4CPV/Ul2JVrba53/oGBrTO9lYaqpMvRoR0jrW4eHJmz+9GeVXnePt9i8HkXj8+8WHzexdLI593f3zer45qpMjCYL7vqbO/Ol1vqbK8REf1kIwM9HrgKeNsuXqtM1ndg3qqZgdgOxJIkSZqgmcLABmCM+s2AllTtN6WIOBK4DjgRuBJ4WUpptGqXSqfhqa61OaU0rz9h18xAbDMhSZIkTdA0YSCltA24h2zW4ckcAaxOKa2b6jwR8TiyiceOBL4InJFSmtgc6G5g22TXyicjO4RsIrJ5zcqAJEmSptI0YSB3LbB/RNSM5BMRB5KN7vOzqQ6OiKOA7wL7kc1i/KoJFQEA8nU/B06IiIkNsE4BFpBVFua12sqAYUCSJEm1mi0MXJIvL4iINoCIKAEX5us/M+lR2X5twOVAP/CxlNLbKhOXTXGtbuC9VefoBN6Xv/zXWb2Dvai2MmAzIUmSJNVqptGESCldHRFfAc4CrouIa4AnAqcCV5CNDARARJyXH3NevurFwElkIwFtrmyf4MGU0qfz3z8PvAp4S0QcB/wSeDZwPPDhlNJvGvrm9oCu9hIlsp7OI9vLjI6V6WgrzfVtSZIkaZ5oqjCQO5tscrBzgDcD9wLvBi6e8E3/e/LlefnytHzZDZxb59w3Ap+G8cnFnk1WGTgTeDLZJGRvBP65Ae9jjyuVSvR0tjGYNxEaHt1OR1czPnJJkiTtCaVyeaqWMtodq1dvmpM/bvWYtc/81HWsH8zmG/j2X/4B+y6sN1qqmpVjUheLz7t4fObF4vMulgbPMzCr5h/N1mdAu6ins6oTsf0GJEmSVMUw0OJ6Oqo6ETuikCRJkqoYBlpcbWXAMCBJkqQdDAMtzlmIJUmSVI9hoMV1OwuxJEmS6jAMtLjqysCwlQFJkiRVMQy0uB4rA5IkSarDMNDi7DMgSZKkegwDLc7KgCRJkuoxDLS42sqAYUCSJEk7GAZanDMQS5IkqR7DQItzBmJJkiTVYxhocVYGJEmSVI9hoMVZGZAkSVI9hoEW11tTGTAMSJIkaQfDQIvrrh5a1HkGJEmSVMUw0OJqhha1MiBJkqQqhoEW12NlQJIkSXUYBlqclQFJkiTVYxhocdVDiw5bGZAkSVIVw0CLqx5adNChRSVJklTFMNDinHRMkiRJ9RgGWlz3hEnHyuXyHN6NJEmS5hPDQIvraCvR2V4CoAxs224YkCRJUsYwUAA9HQ4vKkmSpJ0ZBgqgtt+AnYglSZKUMQwUQM1cA1YGJEmSlDMMFEDNLMRWBiRJkpQzDBSAlQFJkiRNxjBQAN1WBiRJkjQJw0AB1FYGDAOSJEnKGAYKoHpo0WFnIZYkSVLOMFAANUOLWhmQJElSzjBQADXNhKwMSJIkKWcYKICaoUWtDEiSJClnGCgAKwOSJEmajGGgAKwMSJIkaTKGgQKorQwYBiRJkpQxDBRA7WhCNhOSJElSxjBQANXzDFgZkCRJUoVhoACsDEiSJGkyhoECsDIgSZKkyRgGCsAZiCVJkjQZw0AB1FYGbCYkSZKkjGGgALqrKgPDNhOSJElSzjBQADXzDNiBWJIkSTnDQAHUzEBsZUCSJEk5w0ABWBmQJEnSZAwDBdBdFQa2bS+zfaw8h3cjSZKk+cIwUAClUqmmOmAnYkmSJIFhoDBq+w3YVEiSJEmGgcKo7TdgZUCSJEmGgcKomYXYyoAkSZIwDBRGzSzEVgYkSZKEYaAwrAxIkiRpIsNAQVgZkCRJ0kSGgYKorQwYBiRJkmQYKIxuZyGWJEnSBIaBgqidZ8DKgCRJkgwDhdFjZUCSJEkTGAYKoroyMGxlQJIkSRgGCsPKgCRJkiYyDBSEfQYkSZI0kWGgIGorA4YBSZIkGQYKwxmIJUmSNJFhoCCcgViSJEkTGQYKwsqAJEmSJjIMFISVAUmSJE1kGCiI2sqAYUCSJEl7IQxERMeevoamV1sZsJmQJEmSGhgGIuKREfHBiCjlrw+LiOuA4YhYGRF/3qhraddZGZAkSdJEDQkDEXEC8Gvgr4FD89X/CjwB+D3QBXwuIp7biOtp1zkDsSRJkiZqVGXgXfm5zgTui4jDgD8CrgOOAQJYCby1QdfTLqqegXjYyoAkSZJoXBh4MvDllNJ/pJTGgBfk6y9LKZVTSuuBrwMnNeh62kUTKwPlcnkO70aSJEnzQaPCwGLgwarXzwHKwHer1o0ApQZdT7uoo72N9rbsz7+9DKNjhgFJkqSia1QYuBt4NEBE9AFPAe5MKf2+ap+n5ftpjtRWB2wqJEmSVHSNGvbz28BfRcTnyfoH9ACXAUTEE8j6FBwPvLNB19Ms9HS2s2Vb1nl4aHQ7fQ17/JIkSWpGjfo0+C6yEFAZPvQ64OL89zOA5wFXAB9r0PU0C1YGJEmSVK0hYSCltAV4bkQ8GmhLKf2mavOXgK+mlK5vxLU0e7VzDTi8qCRJUtE1tJ1ISumWSVb/NqU02qhr5DMavwl4LXAEsAr4PHBRSmlkF8/1fOAbwAkppRsm2X4p8Io6h38wpfSOXbneXKudhdjKgCRJUtE1LAxExCOB1wDvSCmV87kGvgycEhGrgHNTSl9swKU+CbwOuBa4EngScD5Zn4SX7cL9HksWIqZyPPAQ8OlJtl0702vNF1YGJEmSVK0hYSCfgfgnQC/wKeAedsxAfDuwD9kMxKtTSv+9G9d5IlkQuAI4Mw8dJeALwCsj4vkppatmcJ6nAl8Blk+xTyfZhGlXpZTOm+09zydWBiRJklSt2WYgfkO+fG9KqQyQL99JNq/Ba6Y6OCJ6I+KzwNX5/f5qit2PBTqBm3bznueN2sqAYUCSJKnomm0G4tOANSmlm6tXppQeAG4DTp/m+BXAq4FvkjUB+s0U+z42X7ZOGJgwC7EkSZKKrVF9Bvb4DMQR0Q0cDPy8zi53Z7tFf0ppdZ191gNPTin9ND/nVJeshIGIiJ/mrwfJgsS5eQCZ0tKlC6bbZY/oyD/0T7z+4oXd47+3dXbM2f2pseo9b7Umn3fx+MyLxeddLPPheTfTDMTL8uVAne0b8uWSeidIKW2oBIEZqISBvwfuAj5DVn04B/hFRBw8w/PMG72dO/oMDFoZkCRJKrxmmoG4M18O19leWd+zG9eoNkjW+fkl1UOmRsS5wPuBjwMvneoEAwNbG3Qru6aSLne6/tiOfgLrNw3N2f2pseo+b7Ukn3fx+MyLxeddLI183v39fbM6rplmIB7Ml111tlfawGzZjWuMSym9pM6mC8n6HbwgIhallDY34np7gzMQS5IkqVozzUC8ARijfjOgJVX77TEppbGIuJFswrODgVv35PUaqaeqmdCw8wxIkiQV3h6ZgTjvN7AAWJtSashoPCmlbRFxD9mH8MkcAaxOKa3b3WtFxALyDsMppRsn2aU3Xw7t7rX2pl4rA5IkSarSqA7ERERHRJwbEXeSdfJ9ABiOiBQRfxcRjQge1wL7R8TRE659IHA08LMGXANgf7KmTpdO3JAHhROB1WSTqzWN6sqAMxBLkiSpIWEgIrrIJvI6n2ws/xuBb5F9OD8YeB/wvYhor3uSmbkkX14QEW35tUtk7fghG/Fnt6WU7iSbkOy4iHh5ZX1+rYuAfuCfKxOfNQv7DEiSJKlao5oJvY1sQrDLgLeklNZUNuRNhj4OvBL4K+AfZ3uRlNLVEfEV4Czguoi4BngicCpZB+VvVl33vPyY82Z5udcBPwQujYgzyIZFPZVs4rQfAxfM8rxzpnYGYisDkiRJRdeoZkKvIJvN98+rgwBASmkT8BrgZnaMNrQ7zgbeDSwH3kzWpOfdwCsmfFP/nvxnVlJKvwROJgsZpwFvIJtc7d3AM1NK9YY4nbd6Ou1oeG8AACAASURBVKqaCVkZkCRJKrxGVQYeQdZsZtJPmCml7RHxA+C1u3uhlNIIWbOj902z37SzHaeUziGbRKze9luBM3ftDuev2sqAYUCSJKnoGlUZ2EL2Df1UVlB/wjDtBbWVAZsJSZIkFV2jwsC1wIsj4vjJNkbECcBL8v00R6wMSJIkqVqjmgl9AHgu8MOI+CjwE7LJvw4Cngy8nix4vL9B19MsWBmQJElStUbNQHx9RJwJfI6s0251R94SWTA4uwGzEGs3WBmQJElStYbNQJxS+npEfB94MXA82cg7m4AbgK/nowppDnVVzTMwPDrGWLlMW2naftaSJElqUQ0LAzA+jOilTDJzr+ZeW6lEd0cbw3lVYNvoWM2sxJIkSSqWWYWBiHjhbC+YUrpytsdq9/VUhYGhEcOAJElSkc22MvB1avsFzEQpP8ZPn3Oop7OdDUOjQGUW4s65vSFJkiTNmdmGgfPZ9TCgeaCnqt+AsxBLkiQV26zCQErpvAbfh/aS6mZBWWVAkiRJRdWoScfUJKwMSJIkqcIwUDC1cw1YGZAkSSoyw0DB1M5CbGVAkiSpyAwDBeMsxJIkSaowDBRMbWXAZkKSJElFZhgoGCsDkiRJqpjtPAM1IuKVM9htO7AVuA+4MaU00ohra9d0V1UGBq0MSJIkFVpDwgDwBWonIStV/T7Z5GTrI+JNKaXLG3R9zZCVAUmSJFU0qpnQM4C7gFHgs8A5wDOBlwH/AGwC1gBvBt4PbAEuiYjTG3R9zVDtPANWBiRJkoqsUZWBPwJWAKeklG6csO1rEfFF4GfAviml90TEPwE3A38D/KhB96AZqJ6BeNjKgCRJUqE1qjJwNnD5JEEAgJTSLcAVwKvz12uA/wRObtD1NUNWBiRJklTRqDCwBNg2zT5bgX2rXq8HFjXo+pqh6sqAfQYkSZKKrVFh4HfAiyNi+WQbI2Jf4EVAqlp9HLCyQdfXDNVWBgwDkiRJRdaoPgMXA/8O/DQiLgCuBx4EFgOnAOcCB+RLIuLdwLOBjzfo+pqh2tGEbCYkSZJUZA0JAymlKyLiLcCFwOcm2WUEeGdK6YsRsR9wHtnoQxc34vqaudoZiK0MSJIkFVmjKgOklD4WEVcAZwEnAcuBjcAvgctSSvfmu44Cfwx8O6W0pVHX18xYGZAkSVJFw8IAQErpfuAj0+yzDviPRl5XM2dlQJIkSRUNDQMR8WTgL4DjgQXAWrL5BL6UUrq2kdfS7DgDsSRJkioaNZoQEXEh2QRi5wAnAIcAfwi8DvhRRHygUdfS7NVWBmwmJEmSVGQNCQMRcRbwduAW4PnA0pTSIrLqwDOBm4B3RMSLGnE9zZ6VAUmSJFU0qpnQXwGrgKemlNZWVqaUhoGrI+KZwA35fv/VoGtqFjraSrSXYHsZto+VGd0+Rkd7wwpEkiRJaiKN+hT4WOCq6iBQLaW0GriKrPmQ5lCpVHIWYkmSJAGNCwOlGe7X2aDraTd018xCbL8BSZKkompUGLgReH5ELJtsY0QsB15A1ndAc8zKgCRJkqBxYeDjwAHAdyLi9IjoAIiIxRHxXOD7wArgEw26nnZDT01lwDAgSZJUVA3pQJxS+kpEnAy8FfgBMBYRQ2SjCUHWjOgjKaXLG3E97Z7ayoDNhCRJkoqqYcPIpJT+Gjgd+AJZs6FVZCMIfQE4Pd+uecDKgCRJkqDBMxCnlH4C/KSR51Tj1c41YGVAkiSpqGYVBiJi8WwvmFLaONtj1Ri1sxBbGZAkSSqq2VYGBoDyLI4r78Y11SBWBiRJkgSz/2D+Y2YXBjQPWBmQJEkSzDIMpJSe0uD70F5UWxkwDEiSJBVVw0YTUvPocQZiSZIkYRgoJGcgliRJEhgGCqnbyoAkSZIwDBSSlQFJkiSBYaCQ7DMgSZIkMAwUUnVlYNjKgCRJUmEZBgqotjJgGJAkSSoqw0ABOQOxJEmSwDBQSM5ALEmSJDAMFJKVAUmSJIFhoJCsDEiSJAkMA4VUWxkwDEiSJBWVYaCAaisDNhOSJEkqKsNAAXV31FYGyuXyHN6NJEmS5ophoIDa20p0tZfGXzvxmCRJUjEZBgqqehZi+w1IkiQVk2GgoGpnIbbfgCRJUhEZBgrKyoAkSZIMAwVV3Yl42LkGJEmSCskwUFA1w4s6C7EkSVIhGQYKqmbiMSsDkiRJhWQYKKiaDsRWBiRJkgrJMFBQNR2IrQxIkiQVkmGgoKwMSJIkyTBQUFYGJEmSZBgoqNrKgGFAkiSpiAwDBVU7mpDNhCRJkorIMFBQvc5ALEmSVHiGgYLqrp50zMqAJElSIRkGCqqmmZCVAUmSpEIyDBRUTQdiRxOSJEkqJMNAQdUMLeo8A5IkSYVkGCgohxaVJEmSYaCgqisDw3YgliRJKiTDQEFZGZAkSZJhoKBq+gxYGZAkSSqkjrm+gV0VER3Am4DXAkcAq4DPAxellEZ28VzPB74BnJBSumGS7QuAdwJ/ChwE3AV8EvhUSqm8O+9jrlkZkCRJUjNWBj4JfARYC3wMuB84H7h8V04SEceShYh629uBrwLvAlJ+rRHgE8CHZnPj80nNPAMOLSpJklRITRUGIuKJwOuAK4DTUkrvAE4DLgHOyL/pn8l5ngr8CFg+xW5nAc8FPpxSel5+rZOAHwBvjYjjZv9O5l5Ph0OLSpIkFV1ThQHgDfnyvZVmOvnynUAZeM1UB0dEb0R8Fria7L3/apprjQIXVFbkzZDeBZSAV8/yPcwLne0l2krZ7yPby4yONXWrJ0mSJM1Cs4WB04A1KaWbq1emlB4AbgNOn+b4FWQf4r8JHA/8ZrKdIqIbOAW4IaW0fsLmXwBbZ3Ctea1UKtVUB4atDkiSJBVO04SB/AP6wcAddXa5G1gaEf1TnGY98OSU0gtTSvdPsd9hZJ2rd7pWSmk7cB9w9Ezuez6z34AkSVKxNdNoQsvy5UCd7Rvy5RJg9WQ7pJQ2AD+dwbX2ncG1IiI6Ukqj9U6ydOmCGVyq8TrykYKmu/6Crg7Wbc0GYOpa0DVn96vdM9Pnrdbg8y4en3mx+LyLZT4876apDACd+XK4zvbK+p4mu9acqZlrYJvNhCRJkoqmmSoDg/myq8727ny5ZS9dq0zWd6CugYEpN+8xlXQ53fWrWgnx8Pqt9He3199Z89ZMn7dag8+7eHzmxeLzLpZGPu/+/r5ZHddMlYENwBhZM6DJLKnab3dVOg1Pda3NKaWmbmjvLMSSJEnF1jRhIKW0DbiHbNbhyRwBrE4prWvA5e4Gtk12rXwyskPIJiJras5CLEmSVGxNEwZy1wL7R0TNSD4RcSDZ6D4/a8RF8k7BPwdOiIiJNZdTgAXAdY241lyqrgwMWxmQJEkqnGYLA5fkywsiog0gIkrAhfn6zzT4Wt3AeysrIqITeF/+8l8beK05YWVAkiSp2JqpAzEppasj4ivAWcB1EXEN8ETgVOAKssnEAIiI8/Jjzpvl5T4PvAp4S0QcB/wSeDbZZGUfTilNOmFZM6mdZ8DKgCRJUtE0W2UA4Gzg3cBy4M3A/vnrV6SUylX7vSf/mZV8crFnA/8IHAv8/2Th6Y3A22d73vmkegZiKwOSJEnF01SVAYCU0ghZU533TbNfaQbnOgc4Z4rtm4C35j8txxmIJUmSiq0ZKwNqkNrKgM2EJEmSisYwUGBWBiRJkorNMFBgtaMJWRmQJEkqGsNAgdXOQGxlQJIkqWgMAwXmPAOSJEnFZhgosO6ayoDNhCRJkorGMFBgVgYkSZKKzTBQYD1WBiRJkgrNMFBgVgYkSZKKzTBQYNXzDAxbGZAkSSocw0CB1c5AbGVAkiSpaAwDBeYMxJIkScVmGCiw7prKwHbK5fIc3o0kSZL2NsNAgXW0lehsLwEwVoZt2w0DkiRJRWIYKLiafgN2IpYkSSoUw0DB1fQbsBOxJElSoRgGCq5mrgErA5IkSYViGCi4mlmIrQxIkiQVimGg4KwMSJIkFZdhoOC6rQxIkiQVlmGg4GorA4YBSZKkIjEMFFz10KLDozYTkiRJKhLDQMHVDC1qZUCSJKlQDAMFV9NMyMqAJElSoRgGCq5maFErA5IkSYViGCg4KwOSJEnFZRgoOCsDkiRJxWUYKLjayoBhQJIkqUgMAwVXO5qQzYQkSZKKxDBQcNXzDFgZkCRJKhbDQMFZGZAkSSouw0DBVVcGNgyNzuGdSJIkaW8zDBTcAUt6xn+/6YGN/OSOtXN4N5IkSdqbDAMFd+g+vTwz+sdff+B7t7NhcGQO70iSJEl7i2FA/M3Tj2LZgk4A1m7ZxoevuWOO70iSJEl7g2FALO3t5O+e8cjx19/+3cNcc/uaObwjSZIk7Q2GAQFw+lHLee6j9ht/feH3bmf91m1zeEeSJEna0wwDGve2px5J/6IuANYPjnDx938/x3ckSZKkPckwoHGLezo595lHj7+++rY1fC+tnsM7kiRJ0p5kGFCNJx2xjBc9Zv/x1x+8+nbWbrG5kCRJUisyDGgnb37KI1jR1w1kE5Fd+L3bKZfLc3xXkiRJajTDgHayqLuDv3/WjuZCP7pjLd/63cNzeEeSJEnaEwwDmtQTDtuHM44/YPz1h39wB6s3D8/hHUmSJKnRDAOq669OewQHLs6aC20aHuUD37W5kCRJUisxDKiuBV3tvPvZMf76p3et4xu3PDSHdyRJkqRGMgxoSo8/ZClnnXDg+OuPXHMHD24cmsM7kiRJUqMYBjStN5x6BIcs7QFgy7btvP+7t9lcSJIkqQUYBjSt3s523vPsoJS//vk9A/znTavm9J4kSZK0+wwDmpHjD1rCnz3+4PHXH/3Rndy/YXAO70iSJEm7yzCgGfvLJx3G4ct6ARgcGeO8byU2D4/O8V1JkiRptgwDmrGeznbOe3bQlrcXuuH+jZz9pV/xu4c2ze2NSZIkaVYMA9oljz5gMX/5pMPHX68cGOLVl9/Av//6fjsVS5IkNRnDgHbZq55wKB943jEs7GoHYGR7mQ/94A7e/o3fsWnIZkOSJEnNwjCgWXnmMftxyStOJPZbNL7umtvX8Iov/YpbHrTZkCRJUjMwDGjWDt2nl3/708fxx4/bMSnZAxuGeM3lN3D5r2w2JEmSNN8ZBrRbujva+NunH8VFLzh2vNnQ6FiZj1xzB3975W/ZODQyx3coSZKkegwDaoinH93Pl84+kWNX7Gg29MPfr+UVl/6Km1dtnMM7kyRJUj2GATXMwUt7+eyfPI6zTtjRbGjVxmFe8+Ubuex/V9psSJIkaZ4xDKihujra+OunHcUHX/goFnVnzYa2j5X56I/u5K1fv4WVA85aLEmSNF8YBrRHPO2Ry/nS2SfyqP37xtdde+c6Xva563n3f9/KnWu3zOHdSZIkCQwD2oMOWtLLZ//keP7s8QeNr9tehm/97mH+5Au/5O1X/pb00OY5vENJkqRiMwxoj+psb+MtTzmSfznrsZx86NLx9WXgB/m8BG/+2s3ceP+GubtJSZKkguqY6xtQMZx48FI+9cdL+c0DG/ncz+/l2jvXjW/76V3r+Old6zjpkCW86gmHcvKhSymVSnN4t5IkScVgGNBeddyBi/nHlzyG9PBmvvDze/n+bWuojDH0v/dt4H/v+w3HHdDHq55wKE9+xDJDgSRJ0h5UcrjHPWf16k1z8sddunQBAAMDW+fi8rvk7rVb+cL19/Ht3z7E9gl/rUf2L+TMxx3I045ezuKezrm5wSbQTM9bu8/nXTw+82LxeRdLI593f3/frL5BNQzsQYaBmbt/wyCXXr+SK29+kJEJqaCjrcQfHr4PzzpmP047al96O9vn6C7np2Z83po9n3fx+MyLxeddLIaBFmcY2HUPbxrmsl+u5Gs3rmJodGyn7T0dbZx65L4865h+/vDwZXR12Ae+mZ+3dp3Pu3h85sXi8y4Ww0CLMwzM3vqt2/jW7x7mO7eu5rcPbpp0n0Xd7Tz1qOU865j9ePyhS+loK2b/glZ43po5n3fx+MyLxeddLIaBFmcYaIyVA4N899bVfOfWh7lz7eTvadmCTp5+dD9PP3o5jz1wMZ3txakYtNrz1tR83sXjMy8Wn3exGAZanGGg8X6/egvfTVnF4IENQ5Pu09vZxokHL+WUw5ZyyqH7cOTyBS09KlErP2/tzOddPD7zYvF5F8t8CAMOLaqmclT/Qo7qP4L/86TDueXBTXzn1tV8L61m7ZZt4/sMjoyNz10AWdXg5EOXcsph+3DKoUvZf3HPXN2+JEnSvGJlYA+yMrB3bB8r8+uVG/heWs3P7l7HAxuHp9z/0H16OSUPBycdspS+nubOxEV73kXn8y4en3mx+LyLxcqA1ADtbSVOOnQpJx26FMj6GPzi3gGuv2c91987wIah0Zr9710/yL3rB7nixlWUgEcsX8Cj9+/j0Qcs5jH79/GI5QsL2xlZkiQVi2FALefgpb0cvLSXlz72AMbKZW57eDO/uGeAX9y7nhvu38hw1ZClZeCONVu5Y81Wrrz5ISAbvvTY/ft49P59POaAbLmir7ul+x1IkqRiMgyopbWVShyzoo9jVvTxylMOYXh0jJse2JCHgwFufWgTYxMacw2NjvHrlRv49coN4+uWL+zKqwd9HLtiEUf1L2LfBZ0GBEmS1NQMAyqU7o42Tj50H04+dB/eAGzdtp3fPbSJ3z64iZtXbeLmVRt5ePO2nY5bs2UbP7pjLT+6Y+34un16OzmqfyGP7F/IUcuz5RH7LqTbidAkSVKTMAyo0BZ0tfP4Q5by+EOWjq9bvXmYW1Zt4uYHN3HLqo389sHNbB3ZvtOx6wdHuP7eAa6/d2B8XVsp66B81PJFWUjIg8L+i7tps4ogSZLmGcOANEH/om6e8shunvLI5UA2WtHd67Zyy6pN3PLgJm5fvYU71myZNCCMleHudYPcvW6Qq29bPb6+u6ONQ/fp5fBlCzh8WbY8bJ8FHLqsl97O9r323iRJkqoZBqRptLeVOHL5Qo5cvpAXHrc/AGPlMg9sGOL3q7dw+5ot/H71Fn6/Zgv3rR9ksvFkh0fHuH31Fm5fvWWnbfv3dWfhYFkvh+Vh4ZClvezXZzVBkiTtWU0XBiKiA3gT8FrgCGAV8HngopTSyAyOXwacDzwf2A/4HXBxSukrk+x7KfCKOqf6YErpHbN6E2p6baXS+KhFlQoCwODIdu5ck33o//2aLdy2egt3rd3KwGD9/2k+uGmYBzcN87N71tes72ovceCSnvHrHJz/ftDSHg5a0kNnu30TJEnS7mm6MAB8EngdcC1wJfAksg/3xwMvm+rAiFgIfA94HPBV4F7gDODLEdGfUvrEhEOOBx4CPj3J6a7djfegFtXb2c6jD1jMow9YXLN+YHCEe9Zt5Z51g9y9biv3rM+W9w8Msr3O1HTbtpfHmxxN1FaCFX3dHLS0lyP3W8Qh+yxgn642DlzSwwGLe1jmSEeSJGkGmmoG4oh4IvBT4ArgzJRSOSJKwBeAVwIvSCldNcXxfwd8AHhjSumT+bo+4DqyKsMRKaWH8/WdwBbgqpTSS2dzv85ArOmMbB9j5cAQ96zbyt3rtnL3+kHuWbeVlQNDU1YTptPd0cb+fd0csKSHAxf3cMDibg5c0sP+i3s4cHE3yxZ22QSpSfnfd/H4zIvF510szkC8696QL9+bUioD5IHgncDZwGuAumEAeD0TvulPKW2KiA8A/xf4M+Cj+aZjgU7gpoa+A6lKZ3sbR+y7gCP2XbDTts3Do9w/MMR9A4OsHBhk5YYh7h8Y5L6BIR7eNDxp34SK4dEx7lk/yD3rd64qQNYEab++bvbv62ZFzU8PKxZn6xd1N9v/PUiSpF3VbP/anwasSSndXL0ypfRARNwGnF7vwIg4EjgIuCKlNHEYmGvy5ensCAOPzZeGAc2JRd0dxIpFxIpFO20bHh1j1YYsKKzdtp2V6we5Z/VmHtgwxKqNw2waHp3y3Nu2l1k5MMTKgaG6+yzsame/6qCwqJv+RV3093Wz36Iu+hd1s6Snw+ZIkiQ1saYJAxHRDRwM/LzOLndnu0V/Smn1JNuPzJd3TNyQUnowIoaAo6tWV8JARMRP89eDwDeBc1NKD+z6u5Aao7ujjcP3XcDh+y6YtMS4aWiUVRuH8p9hVm0cGg8KqzYOsXFo6rAAsGXbdu5au5W71tYvXXZ3tLF8Ydd4OOhf1M1+fV35um6WL8p+73H4VEmS5qWmCQPAsnw5UGf7hny5BJgsDOw7zfEb82MrKmHg74GvAT8DngCcAzwjIv4gpbRyqhuufEjb2zryGXDn6vrauyZ73kuBQ/ZfXOeILCw8uGEwCwsbdvw8uGGIVfn6oZGxaa89PDrG/RuGuH9D/QoDQF9PRx4UurPKQr6cuG5Rt5WG6fjfd/H4zIvF510s8+F5N1MY6MyXw3W2V9b37Mbx1U9iELgdeElK6ZbKyog4F3g/8HFgVh2LpbnW19NBX08fj1zRN+n2crnMwOBIFhIGhli1cZCHNg7z0MYhHto4zIMbh3ho0xBbhneeeG0ym4ZG2TQ0yp1rdp5noVp3Rxv9i7rZN680LO/rZvnCrvFlf183++bNlRZ0NdP/fUmSND8107+mlZ6QXXW2d+fLep82ZnL8+LEppZfU2e9C4NXACyJiUUppc5395mwkAEciKJY99bxLwIG9HRzYuwgO2LnfAsCWbaOs3rSNhzcPs3rzjuXqfLlmyzbWbtnG6NjMBtYaHh3LOksPTN7xuVpvZxvLFnTlP53ss6AzX2avly3oGl+3uKeT9rbWqDj433fx+MyLxeddLA0eTWhWxzVTGNgAjFHblKfakqr9JrN+wn4TLSYbaWhKKaWxiLiRbCjSg4FbpztGalULuzpYuG8Hh08yGlLFWLnMxsFR1mzZxpottSFhzZZtrMlfr9myjeHR6ZsmVQyOzKyJEmTzMiztzQLDPr2dLO3t2vF7vtxnQef4PktaKDxIkjSVpgkDKaVtEXEP2YfwyRwBrE4prauz/baq/WpExAFkzYtS/noBeYfhlNKNk5yrN19O/ylEKri2UomlC7IP3Uf1L6y7X7lcZsu27azbOsLaLdtYtzULDGsrr/Pf1+XbttWbrW0SY2VYt3WEdVtnNndDCVjc01EVGLpY0tPB0t7OCT8dLMl/X9jVbn8HSVLTaZowkLsWODsijk4pVT7cExEHko0E9I16B6aU7o2Ie4EnR0RbSqn6K8in5Mvr8uX++e+/YUdH4sq1FgAnknVSvmf33o6kilKpxKLuDhZ1d3DoPr1T7lsul9k8vJ21W7exfusI67duY93WEdZvHdlp3bqtI9MOtbrT+YENQ6NsGBrlbqZvsgTQ0VYaDwlLejtY0lO97GRJT0fNcmlPJ309HVYgJElzqtnCwCVkk4tdEBFn5k12SmTt+AE+M83xlwLnAm8k6wBcmYH4XLI+BZcCpJTujIhfASdGxMtTSpfl+5aAi4B+4PzKxGeS9q5SqZR3gu7g8GXT7z+yfSwPCCOsH9zG+sHs94EJy/WD2e8zGXp1otGx8nhzpxm/D7LO3Et6Oljc08ning4W92QBYnEeGqpfL+7pYKwzW0qS1AhN9S9KSunqiPgKcBZwXURcAzwROBW4gmwOAAAi4rz8mPOqTnExcCbwsYg4nWzOgTOARwBvmjA/weuAHwKXRsQZZPMYnAqcBPwYuKDhb1DSHtHZ3sZ++fClMzG6fYyBoVEGto6wYSgLCDt+Rsd/31C1fnAGQ7FOVAY2Do3m4WPXWh0u7GpnSU8HfVVBYXFPB33dnfn6jvHl4p5O+rqz7Qu62mmzOZMkKVcql5vry+2I6ATeQTbe/0HAvWTf6F+cUhqu2q8MkFIqTTh+BdkH+RcAC8k6AH8opfTlSa51DHA+8DSgjywQfGnitepZvXrTnPxxHYmgWHze88PQyHY2DI2Oh4QNQ6P5coQNeYCo/F5Z7mrzpUZoK0Ff3hwrCw9ZYKiEhcnWL+rOX3d30JWPia29x//Gi8XnXSwNHk1oVt/0NF0YaCaGAe0NPu/mNTpWZtNQFhyyCsGOsLCxel0eLDYOZQFiNs2YGqW7oy0LDJWQ0NM+HhgqoWHR/2vvzqMkK8s7jn9r6eplRoYZJCIugB54zAEBFRQ4rIe4BEyM4BKEeJCDIKKCAVEkgREjiwugssSFTRkkxCMYREVFBsUg4pHFIHmYI2sAWWdgZnqvqvzxvrfqdk1Vb9PdNVX39zmnz1v13qeqb83b3VO/uu99b2/oW9Rwf3Fvkb5iXidaz5B+x7NF450tm0IY6KhpQiIi3aSYz7F0oMTSgVaXP9nQ5psPUK5UeeypF1k7PM6LIyEwrK2Fh3qISIJDcn/dSJnBseldKK6VkfEKI+NhpafZKORzLC4VauFhcW+BxaXYxgDRfHu9RoFCRGTuKAyIiHSYQmrlopkaL1dqIWHdSAgTSZBYm9yO7bqR0BfaMmtHxilP8wJyrZQr1dpKTbNVyMGi3iKLSgUWxSCRbhfFMJFuB0qF2mMWx9sKFSIiCgMiIplSLORnfDQiUa1WGR6vsHY4HRLqIWLdSDkVIMLtdaNx22i4P5MLy7VSrqZPvJ7y9K2W8jlq4WFRDBIDSViItycEiZ5QN1Cqh46wvUipkFOwEJGOpDAgIiLTksvl6O8p0N9TmPbKTI1GxysbBIT1SXgYHWd9bNdNCBSxHRln/Wh5TgIFhIvRJWGGtRv3XIV8LgSDnsKEEDGQhIqe5H7Dtp58vaYU/m0XlQr0FHSitogsDIUBERFZMKVinmXFEstmcWQiMV6usH50YnhYP1IOfTEwJOFhcKwct4X+dM1chQoI05/qRys2Xk8hVwsWL+nrYaC3QG8+hLGBVOhovJ/u6++p9/f1FCjqAnci0oTCgIiIdJRiIc+S/jxLZnHORFoSKsJXPVgM1vrKDKZCxPqRMoNj47Ets36kvm18I8+laDRWrvJCOZxb8eSLs58KldZbzMeAkKc/Boj+hlDRB/dQvgAAE4NJREFUVywwUMrXjgAlQWKgp97X31Ogv1SgP/bpuhUinU1hQEREMmmuQgWE6U+Do2XWj4UwMVgLE/H2WAgW6f6hsVTNWL12cHSc8jwsTB1WgqqwZmhunzcdMvpSAaK/mK/f7qmHhw23pfpToaO3mFfQEFkACgMiIiIbqVTMUyrm2ZyNDxbVapXRcrV2VKLQ28PgaJmnV69nqCE4JIFiaKzM4GiFwRhGhsYqsa9ca+frwjfzFTIA+or5WkhIh4q+YvO+vlqwiG3s6+sp0FecuE1hQyRQGBAREdmE5HI5eos5eosllg6kLkq0ZHYnbUMIGCPjlVqQmBAUxioMjo4zNFZheKx+pGJ4LNQPxbpQG4JG8ti5PO+imeHxCsPzFDQgHNWYEDgawkNjfxIi0ttDm3psqu0t5inoXA3ZxCkMiIiIdLlcLhfeuPYUWDYwd89bqVZroWF4rB4yhpMjE7X+yoTbQw23h8bqjxkaKzMcjzbMt+SoxsZc92IqPYVcKiDk6U3d7kvCRer2ksW99PcUqI6XG2qSIDLxdrKtR8vbyiwpDIiIiMis5HO52ipGc61cqTI8Xg8JtXa8XDuK0RggkrqJNRO3LWTYgHAy+Fh5nLVzcx54SzmoHbXoLeZrIaO3WKA3CR9Jf6qmN6mZ8Jh6yOjtmViXbC/mFT66hcKAiIiIbHLCtRuKLJr9KrSTSo5qJIEj3Q6NVRiJoSE5glHrj33DqbZeu+Hzzde5Go2q1KdVLYR8DkqFVHDYIGCkQkahsT+cY9OXqimlwkhyu1RI3y/o4n7zRGFAREREMmc+j2okknM1kq/hZkGioY9igZGxMmvWjdQek9TU74egEe6H23O9vO1UKtWFDR+J3lRQSIeKUqEeIhpr0vdLqcCRfo4ktJQaHleK/YUuPhKiMCAiIiIyD9LnakxX7YTxNYMz+l7lShI8kqMVqbDQNFCE2glhZYPtE/tHUo+bj+VvpyPZh4WWPhJSSoePJEDEkLG4t8ihO7+cXV+5ZMH3cbYUBkREREQ6XCE//0c60sYr1YlhIhU+kr7RcmVCuBhtEkAa60YbaodT28falUCY2ZGQOx9dw43HvKVjVpJSGBARERGRGSnmcxTn8ZyOZsqVKmPliaFhpFwPD6NJoCingke5yshYuRYoRiZsD4EmuT/xsRPrZzILa+vNeumQHAAoDIiIiIhIByjkcxTyM5t2NVfGK9Va4BgpT2zTt/P5HLu9avOOOr9AYUBEREREZBLhSMjCTcNaSPl274CIiIiIiLSHwoCIiIiISEYpDIiIiIiIZJTCgIiIiIhIRikMiIiIiIhklMKAiIiIiEhGKQyIiIiIiGSUwoCIiIiISEYpDIiIiIiIZJTCgIiIiIhIRikMiIiIiIhklMKAiIiIiEhGKQyIiIiIiGSUwoCIiIiISEYpDIiIiIiIZJTCgIiIiIhIRikMiIiIiIhkVK5arbZ7H0REREREpA10ZEBEREREJKMUBkREREREMkphQEREREQkoxQGREREREQySmFARERERCSjFAZERERERDKq2O4dkLljZkXg48CHge2AJ4HLgXPcfayd+yZzw8y2Bu4HznD3C5ps/yDwSWAHYDVwLXC6u69b0B2VjWJmWwHLgYOBlwHPA78gjOWDDbUa8w5nZlsAZxDGe2vgIeAK4Dx3H2+o1Xh3GTP7MnAScIC7r2zYpvHuAmb2eeBfWmz+D3f/x1Ttgo+5jgx0l4uA84DngK8CjwNnAt9r507J3DCzxcAPgM1abD8VuJLwe/114B7CH5SfmVlpofZTNk4MAr8DjiUEv6/G+x8A7jSz7VO1GvMOZ2YvAW4jfJBzH3Ah8AJwLnCdmeVStRrvLmNmbwZObLFN4909dgFGgM81+fp+UtSuMdeRgS5hZnsBxxB+qN7n7tX4n8gVwAfN7J3u/qN27qPMnpltQwgCb5xk+5nA7cB+yZEgMzsT+FfCz8aFC7O3spGWA68CTnL385JOMzsC+C7wFeDvNeZd41TgdcAJ7v61pNPMrgYOAw4CbtR4d5/45u4yoNBkm8a7u+wM/Mndl7cqaOeY68hA9zg+tp9z9ypAbE8FqsDR7dox2ThmdiLwR8InC79sUXYMIdyf1TAl7CzgRTT+neTdwDPAhGlg7n4V8Gfg7WaWR2PeLbYFHgMubui/JrZ7xlbj3X1OA7YnTAFspPHuEma2GbANcO8UpW0bc4WB7rEv8Ky7/0+6092fAB4A9mvLXslcOBF4hDDG321Rs29sV6Y73X2Y8CnDLma2ZL52UOaGmRUIf/iXu3ulSckIUAJ60Jh3BXf/gLu/uvHcAMLRAoCnYqvx7iJmtjPhw7qzCdPDGmm8u8fOsZ0qDLRtzDVNqAuYWS/wSuCOFiUPhzLb0t2fWbAdk7lyLPALdy+b2Q4tal4LPNXiBKOHY7sDcOc87J/MEXcvE84R2ICZvY7wBvHP7j5iZhrzLhOndm4JvIcwl/hR4Kq4WePdJWLovxRYRQj/X2xSpvHuHkkY2NLMfg7sFu/fDJzm7h7vt23MdWSgOyyL7ZoW21+IrT5F6EDuflN8kziZLdD4d604LehCwt/sb8ZujXn3OZNwJOAiwhi+zd1Xx20a7+5xMuH8r6PdfbRFjca7eyRh4GTCdJ9vET68PRS4w8x2jdvbNuYKA92hJ7YjLbYn/X0LsC/SHj1o/LtS/LT4G8CBwO+pn0ugMe8+DxJXESIcIfi1mSWLBmi8u0A8urscuNjdb5+kVOPdPcqEqb5vdfdD3f0Ud38HcAThzf1lsa5tY65pQt1hKLatlp3qje36BdgXaY8hNP5dJ1475FvAkYQ3iu9KfZKoMe8y7n55ctvM3gn8F/AdM3s9Gu+OF4P9pcDThPMFJqPx7hLufjz1RV7S/SvM7BhgXzMz2jjmOjLQHV4AKrQ+fLQkVSfdaTUa/65iZgPADwlBYBXhgkRPpEo05l0sLgV9M7AjYS6xxrvzHQ/sDRw3jQtIabyz4Q+x3Y42jrnCQBeInxQ+QvhhamY74Bl3f37h9koW2APAy8ysv8m27QhhcdXC7pLMlpktJSwjexBwF7C3uz/aUKYx73BmVjSzvzGzt7YoeSS2L0Xj3Q3eE9sbzayafAEnxP5bYt+2aLy7Qvwd393M3tKiJBnfYdo45goD3eM2YKvG1WbMbGvC2ee/bcteyUK5jfD7vE+608z6gD2A+9x9bTt2TGYmjtmPgLcAtwL7u/vTTUo15t3hBmBFXGGm0S6E68Q8hMa7G1xB8yvQJisBXhnvr0Hj3S0KwG+AnzT+jsdpY3sB48DdtHHMFQa6x3die1ZceST5QTs79n+z6aOkW1xNOElpeVxqNvFZYDM0/p3kLMJ/ELcDf+vuL7ao05h3uHhtgR8QThb+VHqbmR1HWILwRnd/Co13x3P3K9x9eeMX9Q/rku1r0Hh3BXcfIQT+pcBnGjafBLweuLrdY56rVqvz9dyywMzsGuD9wO+AWwhvKPYBvg+8L7kysXQuMzsSuBz4pLtf0LDtHODTwP2EPz47AgcTPpU4MP5Rkk2YmW1FmBpSIqww8ViL0nPcfVhj3vnM7BWEN4OvBG4iXG38DYTVox4iTBF7ItZqvLuQmV1AmCp0gLuvTPVrvLtAnPZ1O7AV4WrT9wBvAvYH/gTs6+7Pxdq2jLmODHSXfwJOJ8wvPZHwg3c6cISCQCacCnyMMK3gBGAn4HzgYP2n0TH2oL6axFHAGS2+kuXlNOYdzt0fB3YnrBq1M+Fv9/aEJWR3bzhpXOOdLRrvLuDuDxOO8l1GGMNPEM4B+AqwVxIEoraMuY4MiIiIiIhklI4MiIiIiIhklMKAiIiIiEhGKQyIiIiIiGSUwoCIiIiISEYpDIiIiIiIZJTCgIiIiIhIRikMiIiIiIhklMKAiIiIiEhGKQyIiIiIiGSUwoCIiIiISEYpDIiIiIiIZFSx3TsgIiLTY2ZHApdPp9bdc/O7N9NnZlXgHnffdQ6ea2/gp8Bfu/tjG/E8i4D7gOvd/cQWNQcD/wLsBAwBNwCnuvvTTWr3BD4PvAmoAjcDn3b3B1M1LwVWAYe7+49nu+8iInNJYUBEpPPcCqxs904sNDPrA74NXLCRQaAIrAC2maTmMOBq4EHgEuDVwJHAfma2m7uvSdXuB/wMWA1cASwBPgAcEGsfBnD3Z83sbOASM9vR3dfN9jWIiMwVhQERkc6z0t2Xt3sn2uDTwMuBL8/2CcxsGXAN8NZJahYDFxGCwBvc/cXY/zPgUsLRgpNjXx74BjAI7Obu/xf7VwA/j/v6ntTTfx34JLA8eQ4RkXbSOQMiIrLJM7MB4BPAd9Kfys/wOQ4D7icEgZ9PUnoYsBQ4PwkCAO5+GeDAkWZWiN0HAgZcmgSBWHtz/B7/YGZbpPqHCIHiI2a2dDavQ0RkLunIgIhIFzOz/YFbgA8DJeAkwqfrqwhvdq9o8pj3E95470qY/34v8DV3v6ZJ7b7AKcAeQA9hHv657v7DJrV7AP8G7AmMAL8ETk6m0UzhSGAZcFXq+bYH7gHKwOvc/fHUtpuAtwFHuPuK2H0sYe7/3wHraH10YN/Y3tJk28r4PDvF7z1Z7S1xH/YG0v8eK4DTgI8AZ7fYBxGRBaEjAyIi2XAc8DXgDuAyYAvgcjNbni4ysy8TptG8hjBn/nvAdsD3zOzchtojCG/o9wV+Ep/3VcD1Zvahhu+/LfXzHC4E7gYOBX4VT+adymHA88CdSYe7ryJM2VkcX1uyX8cS3oRfmwoCAGcSQsOPpvher43tg022PRzbHRpq/zyN2mS/7wceI7wmEZG20pEBEZHOs3/jm/gGP3X33zb0vRF4r7t/H8DMzgRuB04zsxXuvsrM9iEcObgLeLu7PxNrtyS86T/FzG5091/FKS4XAs8B+7j7A7H2C4RPzL9kZle5+1j8/kuA09z9rGSHzOx64F2EN+7XtXoxZtYPvBn4tbtXGjZfQJiTf4iZHQT8iTBP/0lCAKpx919O8m+WtgUwEqf0NHoh9XqSWoBmU5caa9N+T5xC5O7PTXO/RETmnI4MiIh0nv2AMyb52qPJY36TBAGAuDzmWYQPhd4Xu4+M7clJEIi1zwCfiXePiu1BhDe5X02CQKx9lnCC7BcJn9gnhtjwxN8bYvuaSV9tmJJTIrzRnyCGgw8Bw4Rg8O34fY9y9+eneN5WegjTmJpJ+vtSten+yWrT7gNyhJAmItI2OjIgItJ5PjeL1YRubdL3u9juEttdgQpwW5Pa2xpqk/b2xkJ3v7bJ4x9199GGvuQT8cWNxQ3+KrbPNtvo7m5mpxMCyPbAJe7+0ymeczJDhPDRTG9s16dqaVHfWJuWvJYtZ7x3IiJzSEcGRESy4fEmfX+JbTKNZTNguMmbdtz9BcLymQOxK1kJ58XG2haGJ9k21QXSkv0bnKTmOsLJzgD/Pc19amU10GdmvU22JfvyQqo23T9ZbVoSELSikIi0lcKAiEg29Dfp2zy2yafUa4EBM9u8sTBe8Kuf+qf5yQWzXtKktjde2GuuJNN9mr3hxsxywDfj3TXA+fE8h9lKpj1t22TbdrH1htrtplGblvwbNzsvQURkwSgMiIhkw+5N+vaM7R2xvTu2ezep3ZvwCf598f4fY/vmJrUnA0Pxyrxz4cnYvrTF9uOAAwiB4OOx7qKN+H7JlKhm+78/4ZP++6dZW6E+HSsteS2zvpKyiMhcUBgQEcmGQ8ys9ibfzLYiLMu5Hkjm+F8R27PTn6zH21+Kd78b2+vjY08ws21StcsI6/CvBRpXNJqtVYSTcXds3GBm2wLnEqY8fcbdrwJuBt5rZofO8vtdT9j/U+LrSb7XUYRlQr+dWtXoVuBR4Ni4L0ntgYTrGFyXPhk7ZafY3jPLfRQRmRM6gVhEpPNMtbQowDXu/r+p+4PAzWb2n4R5/u8GXgYc4+5/AYhLhp4H/DNwr5klq/28k3ChsnPd/Vex9nkzOx64HLgrLhO6jrDM58uBQ9y91Yo8M+Lug2a2EtjPzErJOQ1xelCyetCHU1cmPo5wobSLzWzlTJfujK/tFOAS4G4zuxZ4BWHVpQcIqzAltWUz+yjhomK/N7MVcX8OJ0y/+lTj88f93gu4N67qJCLSNgoDIiKdZz+aT0tJuxtIh4ErCScRf4ywNv5dhOU3f5J+kLufZGZ/iHWHA2PxuT7m7j9oqL3SzB4HTiWEgB7gD8DR7v7jWb62Vn4IvJ1wgbNfxL5jgAMJ11WoXR05XjPhC8DnCddCmPHFvdz9381sNeHqyscTzlu4knCthOcbam80s3cQlnU9mhCKbgA+6+4PNXn63QgnDp/bZJuIyILKVavVqatERKQjmdn+wC2E6wGc2ObdmbV4leJHgJvc/fB278/GMLOLgSOAbdx99VT1IiLzSecMiIjIJs/d1xMuKnaImW0xVf2mKoaaw4CLFQREZFOgMCAiIp3ifOAJwrSkTnUS4fyNc9q9IyIioDAgIiIdIh4dOAr4qJm9ut37M1NxVaaTgWNTJzuLiLSVzhkQEREREckoHRkQEREREckohQERERERkYxSGBARERERySiFARERERGRjFIYEBERERHJKIUBEREREZGMUhgQEREREckohQERERERkYxSGBARERERySiFARERERGRjFIYEBERERHJKIUBEREREZGMUhgQEREREcmo/wdrRhAUlYsoiQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 273,
       "width": 385
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.40, random_state=42\n",
    ")\n",
    "\n",
    "preprocessing_object = Preprocessing()\n",
    "X_train_std = preprocessing_object.standardize_save_state(X_train)\n",
    "X_train_std = preprocessing_object.insert_bias_term(X_train_std)\n",
    "\n",
    "bcw_model = LogisticRegr()\n",
    "model_params = model_parameter(lambda_ridge=0, alpha=0.5, epochs=5000)\n",
    "\n",
    "bcw_model.fit(X_train_std, y_train, model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the performance of out model with accuracy, precision, recall and the f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficient vector w or theta trained from our model is [-0.68966442  0.21841225  0.40117853  0.28335985 -0.06065617  0.0379248\n",
      "  3.14934633 -1.88541698 -3.52066927  1.23684842 -0.92544834 -3.46479585\n",
      "  0.6850434  -1.07373119 -2.35719124 -0.31702896  0.05820801  1.09554224\n",
      " -1.70401767  1.24173203  1.58056678 -1.39346711 -3.22423253 -0.29002135\n",
      " -1.40487317 -0.21187648  1.29367031 -2.63272468 -1.46349068 -3.41398423\n",
      "  0.04106486]\n"
     ]
    }
   ],
   "source": [
    "print(f\"The coefficient vector w or theta trained from our model is {bcw_model.theta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the logistic regr model is 0.9736842105263158\n",
      "The precision of the logistic regr model is  0.9930555555555556\n",
      "The recall of the logistic regr model is  0.9662162162162162\n",
      "The f1 score of the logistic regr model is  0.9794520547945206\n",
      "The confusion matrix from the results of the logistic regr model:\n",
      "\t\t\t      Actual values\n",
      "\t\t\tPositive(1)   Negative(0)\n",
      "Predicted| Positive(1)     TP 143\t  FP 1\n",
      "  Values | Negative(0)     FN 5\t\t  TN 79\n"
     ]
    }
   ],
   "source": [
    "X_test = preprocessing_object.fit(\n",
    "    X_test\n",
    ")  # Standardize X_test with the same mean and std parameters from X_train\n",
    "X_test = Preprocessing.insert_bias_term(X_test)\n",
    "print(\n",
    "    f\"The accuracy of the logistic regr model is {bcw_model.accuracy(X_test, y_test)}\"\n",
    ")\n",
    "print(\n",
    "    f\"The precision of the logistic regr model is  {bcw_model.precision(X_test, y_test)}\"\n",
    ")\n",
    "print(f\"The recall of the logistic regr model is  {bcw_model.recall(X_test, y_test)}\")\n",
    "print(\n",
    "    f\"The f1 score of the logistic regr model is  {bcw_model.f1_score(X_test, y_test)}\"\n",
    ")\n",
    "print(\"The confusion matrix from the results of the logistic regr model:\")\n",
    "bcw_model.plot_confusion_matrix(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running our logistic regression model with different hyperparameters with K Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARNING EXPENSIVE GRID SEARCH COMPUTATION AHEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/Downloads/Engr_machine_learning/ml_linear_logistic_regr_applied/venv/lib/python3.7/site-packages/ipykernel_launcher.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ccd8699b6b74f68a9cb04a02a20578f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When lambda is 0, alpha=0.4 and epochs=4000\n",
      "Average train loss is: 0.041503485344886945\n",
      "Average test loss is: 0.10682212226529317\n",
      "\n",
      "When lambda is 0, alpha=0.4 and epochs=5000\n",
      "Average train loss is: 0.0402709618684421\n",
      "Average test loss is: 0.11347584817832576\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/Downloads/Engr_machine_learning/ml_linear_logistic_regr_applied/venv/lib/python3.7/site-packages/ipykernel_launcher.py:71: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When lambda is 0, alpha=0.4 and epochs=7000\n",
      "Average train loss is: nan\n",
      "Average test loss is: 0.12492158753830071\n",
      "\n",
      "When lambda is 0, alpha=0.5 and epochs=4000\n",
      "Average train loss is: 0.04026978603046494\n",
      "Average test loss is: 0.11347888444454239\n",
      "\n",
      "When lambda is 0, alpha=0.5 and epochs=5000\n",
      "Average train loss is: nan\n",
      "Average test loss is: 0.12086192335014453\n",
      "\n",
      "When lambda is 0, alpha=0.5 and epochs=7000\n",
      "Average train loss is: nan\n",
      "Average test loss is: 0.13358478041961996\n",
      "\n",
      "When lambda is 0, alpha=0.6 and epochs=4000\n",
      "Average train loss is: nan\n",
      "Average test loss is: 0.11945432144524988\n",
      "\n",
      "When lambda is 0, alpha=0.6 and epochs=5000\n",
      "Average train loss is: nan\n",
      "Average test loss is: 0.1275065932074594\n",
      "\n",
      "When lambda is 0, alpha=0.6 and epochs=7000\n",
      "Average train loss is: nan\n",
      "Average test loss is: 0.14135347938907866\n",
      "\n",
      "When lambda is 0, alpha=0.7 and epochs=4000\n",
      "Average train loss is: nan\n",
      "Average test loss is: 0.1249270150980496\n",
      "\n",
      "When lambda is 0, alpha=0.7 and epochs=5000\n",
      "Average train loss is: nan\n",
      "Average test loss is: 0.13358669589845656\n",
      "\n",
      "When lambda is 0, alpha=0.7 and epochs=7000\n",
      "Average train loss is: nan\n",
      "Average test loss is: 0.14842175617312497\n",
      "\n",
      "When lambda is 0, alpha=0.8 and epochs=4000\n",
      "Average train loss is: nan\n",
      "Average test loss is: 0.13000159538796147\n",
      "\n",
      "When lambda is 0, alpha=0.8 and epochs=5000\n",
      "Average train loss is: nan\n",
      "Average test loss is: 0.13921332527693014\n",
      "\n",
      "When lambda is 0, alpha=0.8 and epochs=7000\n",
      "Average train loss is: nan\n",
      "Average test loss is: 0.15491182141033627\n",
      "\n",
      "When lambda is 5, alpha=0.4 and epochs=4000\n",
      "Average train loss is: 0.09847437548118096\n",
      "Average test loss is: 0.32214701131670864\n",
      "\n",
      "When lambda is 5, alpha=0.4 and epochs=5000\n",
      "Average train loss is: 0.09847437548118095\n",
      "Average test loss is: 0.32214701147157887\n",
      "\n",
      "When lambda is 5, alpha=0.4 and epochs=7000\n",
      "Average train loss is: 0.09847437548118095\n",
      "Average test loss is: 0.3221470114736781\n",
      "\n",
      "When lambda is 5, alpha=0.5 and epochs=4000\n",
      "Average train loss is: 0.09847437548118093\n",
      "Average test loss is: 0.3221470114717492\n",
      "\n",
      "When lambda is 5, alpha=0.5 and epochs=5000\n",
      "Average train loss is: 0.09847437548118093\n",
      "Average test loss is: 0.322147011473671\n",
      "\n",
      "When lambda is 5, alpha=0.5 and epochs=7000\n",
      "Average train loss is: 0.09847437548118095\n",
      "Average test loss is: 0.3221470114736783\n",
      "\n",
      "When lambda is 5, alpha=0.6 and epochs=4000\n",
      "Average train loss is: 0.09847437548118095\n",
      "Average test loss is: 0.3221470114736588\n",
      "\n",
      "When lambda is 5, alpha=0.6 and epochs=5000\n",
      "Average train loss is: 0.09847437548118095\n",
      "Average test loss is: 0.32214701147367836\n",
      "\n",
      "When lambda is 5, alpha=0.6 and epochs=7000\n",
      "Average train loss is: 0.09847437548118096\n",
      "Average test loss is: 0.3221470114736783\n",
      "\n",
      "When lambda is 5, alpha=0.7 and epochs=4000\n",
      "Average train loss is: 0.09847437548118095\n",
      "Average test loss is: 0.3221470114736785\n",
      "\n",
      "When lambda is 5, alpha=0.7 and epochs=5000\n",
      "Average train loss is: 0.09847437548118095\n",
      "Average test loss is: 0.3221470114736783\n",
      "\n",
      "When lambda is 5, alpha=0.7 and epochs=7000\n",
      "Average train loss is: 0.09847437548118095\n",
      "Average test loss is: 0.3221470114736783\n",
      "\n",
      "When lambda is 5, alpha=0.8 and epochs=4000\n",
      "Average train loss is: 0.09847437548118095\n",
      "Average test loss is: 0.3221470114736784\n",
      "\n",
      "When lambda is 5, alpha=0.8 and epochs=5000\n",
      "Average train loss is: 0.09847437548118095\n",
      "Average test loss is: 0.3221470114736784\n",
      "\n",
      "When lambda is 5, alpha=0.8 and epochs=7000\n",
      "Average train loss is: 0.09847437548118095\n",
      "Average test loss is: 0.3221470114736784\n",
      "\n",
      "When lambda is 10, alpha=0.4 and epochs=4000\n",
      "Average train loss is: 0.11964007306923016\n",
      "Average test loss is: 0.40585547068907735\n",
      "\n",
      "When lambda is 10, alpha=0.4 and epochs=5000\n",
      "Average train loss is: 0.11964007306923016\n",
      "Average test loss is: 0.40585547068907707\n",
      "\n",
      "When lambda is 10, alpha=0.4 and epochs=7000\n",
      "Average train loss is: 0.11964007306923016\n",
      "Average test loss is: 0.40585547068907707\n",
      "\n",
      "When lambda is 10, alpha=0.5 and epochs=4000\n",
      "Average train loss is: 0.11964007306923016\n",
      "Average test loss is: 0.4058554706890772\n",
      "\n",
      "When lambda is 10, alpha=0.5 and epochs=5000\n",
      "Average train loss is: 0.11964007306923016\n",
      "Average test loss is: 0.4058554706890772\n",
      "\n",
      "When lambda is 10, alpha=0.5 and epochs=7000\n",
      "Average train loss is: 0.11964007306923016\n",
      "Average test loss is: 0.4058554706890772\n",
      "\n",
      "When lambda is 10, alpha=0.6 and epochs=4000\n",
      "Average train loss is: 0.11964007306923016\n",
      "Average test loss is: 0.40585547068907724\n",
      "\n",
      "When lambda is 10, alpha=0.6 and epochs=5000\n",
      "Average train loss is: 0.11964007306923016\n",
      "Average test loss is: 0.40585547068907724\n",
      "\n",
      "When lambda is 10, alpha=0.6 and epochs=7000\n",
      "Average train loss is: 0.11964007306923016\n",
      "Average test loss is: 0.40585547068907724\n",
      "\n",
      "When lambda is 10, alpha=0.7 and epochs=4000\n",
      "Average train loss is: 0.11964007306923016\n",
      "Average test loss is: 0.40585547068907724\n",
      "\n",
      "When lambda is 10, alpha=0.7 and epochs=5000\n",
      "Average train loss is: 0.11964007306923016\n",
      "Average test loss is: 0.40585547068907724\n",
      "\n",
      "When lambda is 10, alpha=0.7 and epochs=7000\n",
      "Average train loss is: 0.11964007306923016\n",
      "Average test loss is: 0.40585547068907724\n",
      "\n",
      "When lambda is 10, alpha=0.8 and epochs=4000\n",
      "Average train loss is: 0.11964007306923015\n",
      "Average test loss is: 0.40585547068907724\n",
      "\n",
      "When lambda is 10, alpha=0.8 and epochs=5000\n",
      "Average train loss is: 0.11964007306923015\n",
      "Average test loss is: 0.40585547068907724\n",
      "\n",
      "When lambda is 10, alpha=0.8 and epochs=7000\n",
      "Average train loss is: 0.11964007306923015\n",
      "Average test loss is: 0.40585547068907724\n",
      "\n",
      "When lambda is 15, alpha=0.4 and epochs=4000\n",
      "Average train loss is: 0.1348379172970453\n",
      "Average test loss is: 0.46454796214928085\n",
      "\n",
      "When lambda is 15, alpha=0.4 and epochs=5000\n",
      "Average train loss is: 0.1348379172970453\n",
      "Average test loss is: 0.46454796214928085\n",
      "\n",
      "When lambda is 15, alpha=0.4 and epochs=7000\n",
      "Average train loss is: 0.1348379172970453\n",
      "Average test loss is: 0.46454796214928085\n",
      "\n",
      "When lambda is 15, alpha=0.5 and epochs=4000\n",
      "Average train loss is: 0.13483791729704533\n",
      "Average test loss is: 0.4645479621492809\n",
      "\n",
      "When lambda is 15, alpha=0.5 and epochs=5000\n",
      "Average train loss is: 0.13483791729704533\n",
      "Average test loss is: 0.4645479621492809\n",
      "\n",
      "When lambda is 15, alpha=0.5 and epochs=7000\n",
      "Average train loss is: 0.13483791729704533\n",
      "Average test loss is: 0.4645479621492809\n",
      "\n",
      "When lambda is 15, alpha=0.6 and epochs=4000\n",
      "Average train loss is: 0.13483791729704533\n",
      "Average test loss is: 0.4645479621492809\n",
      "\n",
      "When lambda is 15, alpha=0.6 and epochs=5000\n",
      "Average train loss is: 0.13483791729704533\n",
      "Average test loss is: 0.4645479621492809\n",
      "\n",
      "When lambda is 15, alpha=0.6 and epochs=7000\n",
      "Average train loss is: 0.13483791729704533\n",
      "Average test loss is: 0.4645479621492809\n",
      "\n",
      "When lambda is 15, alpha=0.7 and epochs=4000\n",
      "Average train loss is: 0.13483791729704533\n",
      "Average test loss is: 0.4645479621492809\n",
      "\n",
      "When lambda is 15, alpha=0.7 and epochs=5000\n",
      "Average train loss is: 0.13483791729704533\n",
      "Average test loss is: 0.4645479621492809\n",
      "\n",
      "When lambda is 15, alpha=0.7 and epochs=7000\n",
      "Average train loss is: 0.13483791729704533\n",
      "Average test loss is: 0.4645479621492809\n",
      "\n",
      "When lambda is 15, alpha=0.8 and epochs=4000\n",
      "Average train loss is: 0.13483791729704533\n",
      "Average test loss is: 0.4645479621492809\n",
      "\n",
      "When lambda is 15, alpha=0.8 and epochs=5000\n",
      "Average train loss is: 0.13483791729704533\n",
      "Average test loss is: 0.4645479621492809\n",
      "\n",
      "When lambda is 15, alpha=0.8 and epochs=7000\n",
      "Average train loss is: 0.13483791729704533\n",
      "Average test loss is: 0.4645479621492809\n",
      "\n",
      "\n",
      "The lambda for ridge regression that yields the mininum error is 0\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.disabled = True\n",
    "\n",
    "min_test_error = float(\"inf\")  # tracks the minimum error with k-folds\n",
    "best_lambda = None  # saves the best theta value with the min_test_error\n",
    "best_alpha = None  # saves best alpha value\n",
    "best_epochs = None  # saves best epochs\n",
    "\n",
    "possible_lambda_ridge = [0, 5, 10, 15]\n",
    "possible_alpha = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "possible_epochs = [4000, 5000, 7000]\n",
    "\n",
    "for lambda_ridge in tqdm(possible_lambda_ridge):\n",
    "    for alpha in possible_alpha:\n",
    "        for epochs in possible_epochs:\n",
    "            X_feat = X\n",
    "            bcw_cv_model = LogisticRegr()\n",
    "            kfold_logistic_regr = KFoldCrossValidator()\n",
    "\n",
    "            model_params = model_parameter(\n",
    "                lambda_ridge=lambda_ridge, alpha=alpha, epochs=epochs\n",
    "            )\n",
    "            kfold_logistic_regr.cross_validate(\n",
    "                bcw_cv_model,\n",
    "                X_feat,\n",
    "                y,\n",
    "                k=10,\n",
    "                logging_enabled=False,\n",
    "                model_params=model_params,\n",
    "                custom_kfold=False,\n",
    "            )\n",
    "\n",
    "            lregr_train_loss = kfold_logistic_regr.train_loss\n",
    "            lregr_test_loss = kfold_logistic_regr.test_loss\n",
    "            print(f\"When lambda is {lambda_ridge}, alpha={alpha} and epochs={epochs}\")\n",
    "            avg_train_loss, avg_test_loss = (\n",
    "                sum(lregr_train_loss) / len(lregr_train_loss),\n",
    "                sum(lregr_test_loss) / len(lregr_test_loss),\n",
    "            )\n",
    "            print(f\"Average train loss is: {avg_train_loss}\")\n",
    "            print(f\"Average test loss is: {avg_test_loss}\")\n",
    "            print()\n",
    "\n",
    "            if avg_test_loss < min_test_error:\n",
    "                min_test_error = avg_test_loss\n",
    "                best_lambda = lambda_ridge\n",
    "                best_alpha = alpha\n",
    "                best_epochs = epochs\n",
    "\n",
    "logger.disabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters that yields the mininum error of 0.10682212226529317 are lambda=0, alpha=0.4 and no of iterations = 4000\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Best parameters that yields the mininum error of {min_test_error} are lambda={best_lambda}, alpha={best_alpha} and no of iterations = {best_epochs}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "ml_svm_kernel",
   "language": "python",
   "name": "ml_svm_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
